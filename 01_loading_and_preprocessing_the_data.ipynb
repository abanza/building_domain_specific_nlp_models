{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and Preparing the Dataset\n",
    "Language is very domain specific. Businesses collect text-based datasets that are tailored to their domains (legal, healthcare, insurance, social networks, finance, etc.). These domain-specific corpuses can then be exploited in multiple ways: entity recognition, search indexing, query completion, product recommendations systems, or sentiment analysis, to name just a few.\n",
    "\n",
    "Language models are at the core of many natural language processing (NLP) applications like the ones listed in the previous paragraph. Simply put, given surrounding or preceding words, a language model predicts a word. As you will see, you can directly exploit language models for any application that generates text, such as machine translation, speech to text, text generation, or query completion applications.\n",
    "\n",
    "Off-the-shelf models which are trained on large generic corpuses do not reflect the particularities of a given business domain and corpus. For instance, completing the query \"how to avoid over…\" will not give the same results in the general public domain (overeating, overthinking, …) compared to a data science context where “overfitting” would be a more appropriate completion.\n",
    "\n",
    "To reap the benefits of domain-specific corpuses, we must build language models that are tuned to the particular vocabulary of the domain at hand. Since Stack Exchange operates domain specific forums, the content of these forums constitutes perfect corpuses for building domain specific language models.\n",
    "\n",
    "In this Project, we will be building statistics-focused language models using gradually more complex methods. We will evaluate and apply these models to the tasks of:\n",
    "\n",
    "- Query completion\n",
    "- Larger text generation\n",
    "- Sentence selection\n",
    "\n",
    "At the end of this project, you will be able to build the foundations of any domain-specific NLP system by creating the most a robust and efficient language model.\n",
    "\n",
    "Here we will working the Stack Exchange dataset. \n",
    "\n",
    "The link to the dataset is available here:\n",
    "\n",
    "https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and cleaning the data\n",
    "The goal here is threefold:\n",
    " 1. We want to reduce the noise in the original raw text by removing everything that does not bring information to the language model. Everything that is not exactly text: html tags, math notations, urls, etc.\n",
    " 2. We want to prepare the corpus and make it ready for our language model by tokenizing the text.\n",
    " 3. And finally, we want to remove rows with short or very long texts. As you will see, some of the entries are mostly made of large numerical tables. Entries that are too long will not be good reflection of the corpus. Entries that are too short will not bring relevant information to the language model either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and shuffling it at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz',\n",
    "                   compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812132"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812132, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a small dataset to help with computation on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = copy.deepcopy(full_data[:80000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>208246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I am analyzing data (size/survival) of two ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750748.0</td>\n",
       "      <td>@Tim Does it mean that I am interpreting the N...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374050.0</td>\n",
       "      <td>Well, looking back on that, it is only true th...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>238691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>453902.0</td>\n",
       "      <td>Have you considered using the post-interventio...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153274</td>\n",
       "      <td>72117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Perhaps you're already aware of this, but C...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   208246        NaN         NaN   \n",
       "1   401059        NaN    750748.0   \n",
       "2     2132        NaN    374050.0   \n",
       "3   238691        NaN    453902.0   \n",
       "4   153274    72117.0         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0  <p>I am analyzing data (size/survival) of two ...     post  \n",
       "1  @Tim Does it mean that I am interpreting the N...  comment  \n",
       "2  Well, looking back on that, it is only true th...  comment  \n",
       "3  Have you considered using the post-interventio...  comment  \n",
       "4  <p>Perhaps you're already aware of this, but C...     post  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the types of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment    54421\n",
      "post       16611\n",
      "title       8968\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(small_data.category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "z_star calculated via scipy can grow to infinty - Is this right?\n",
      "*************************\n",
      "Acceptance-Rejection using Functional\n",
      "*************************\n",
      "Is this really perfect separation in logistic regression, or is something else going on?\n",
      "*************************\n",
      "How do I test a hypothesis that proposes existence of a positive relationship between two variables?\n"
     ]
    }
   ],
   "source": [
    "# sample titles\n",
    "for title in small_data[small_data.category == 'title'].text.sample(4).values:\n",
    "    print(\"*\" * 25)\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the posts, we can see that the text have html tags and latex formatted equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "<p>It is possible for one set of data to more closely approximate a given theoretical distribution than another.  One way to think about how close one distribution is to another is to use the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" rel=\"nofollow noreferrer\">Kullback-Leibler divergence</a>.  This is a measure of how much <a href=\"https://en.wikipedia.org/wiki/Information_theory\" rel=\"nofollow noreferrer\">information</a> is lost by using one distribution instead of the other.  More specifically, we can determine the information loss associated with using the densities from a true normal fitted to your data instead of a more flexible <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" rel=\"nofollow noreferrer\">kernel density</a> fitted to your data.  </p>\n",
      "\n",
      "<pre><code>library(flexmix)\n",
      "sim = function(x, ref){     # find similar values\n",
      "  lr   = length(ref)\n",
      "  indx = vector(length=lr)\n",
      "  for(i in 1:lr){\n",
      "    indx[i] = which(abs(x-ref[i])==min(abs(x-ref[i])))\n",
      "  }\n",
      "  return(indx)\n",
      "}\n",
      "get.kl = function(res){     # get densities &amp; K-L divergence\n",
      "  d.k = density(res)\n",
      "  d.k = d.k$y[sim(d.k$x, res)]\n",
      "  d.n = dnorm(res, mean=mean(res), sd=sd(res))\n",
      "  return(KLdiv(cbind(d.k, d.n))[1,2])\n",
      "}\n",
      "\n",
      "get.kl(residuals_1)  # [1] 0.01309022\n",
      "get.kl(residuals_2)  # [1] 0.005502055\n",
      "</code></pre>\n",
      "\n",
      "<p>More than twice as much information would be lost when approximating <code>residuals_1</code> by a normal than approximating <code>residuals_2</code>.  </p>\n",
      "\n",
      "<hr>\n",
      "\n",
      "<p>It is also possible for a misspecified model to yield non-normal residuals:  </p>\n",
      "\n",
      "<pre><code>set.seed(3)\n",
      "x = runif(30, min=0, max=10)\n",
      "g = rep(0:1, each=15)\n",
      "y = 17 - 0.3*x + 2*g + rnorm(30)\n",
      "\n",
      "m1 = lm(y~x)\n",
      "m2 = lm(y~x+g)\n",
      "get.kl(resid(m1))  # [1] 0.04971849\n",
      "get.kl(resid(m2))  # [1] 0.008378342\n",
      "</code></pre>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/zxoZ2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zxoZ2.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<hr>\n",
      "\n",
      "<p>Having said those things, using how closely your residuals approximate a normal distribution to select between models with different covariates would be rather odd.  It is much more typical for people to select between models using a hypothesis test (if the models are nested), the AIC (if they are not nested), out of sample predictive error via cross-validation (if prediction is the ultimate goal of the model), etc.  People do (and should) evaluate the residuals of their models, but they might use that to determine if a transformation is necessary, or if they should bootstrap instead of rely on the standard errors, for example.  </p>\n",
      "\n",
      "*************************\n",
      "<p>In some fields, a lot of sample studies feature statistical tests. Oftentimes, box plots of sample distributions are shown. </p>\n",
      "\n",
      "<p>I was wondering whether i could read distributional parameters from these plots and then generate additional samples from the same population (assuming normal distributions).I understand I am not actually sampling the same population as done in such a study. \n",
      "Still, I assume the best case for such a study would be to get a sampled distribution with identical parameters as the original population.</p>\n",
      "\n",
      "<p>In order to reproduce the probability of type 1 errors for statistical tests done on the boxplotted data, I would generate samples of equal sample size repeatedly and perform significance tests on them.\n",
      "My question is if this approach is valid to estimate the probability of type 1 errors.</p>\n",
      "\n",
      "<ol>\n",
      "<li><p>Is it possible (and statistically valid) to recreate random variables or their distribution of a sample using a boxplot of said sample (assuming normal distribution)?</p></li>\n",
      "<li><p>If the answer to 1. is yes, are there other distributions for which this approach is possible?</p></li>\n",
      "<li><p>Is the best case for a sample study to have a sample with equivalent distribution parameters as the population sampled from?</p></li>\n",
      "<li><p>Can the probability of type 1 errors significance tests be reproduced by performing analysis on generated samples based on question 1?</p></li>\n",
      "</ol>\n",
      "\n",
      "*************************\n",
      "<p>How do you calculate the McFadden's pseudo-$R^2$ using the <code>cv.glmnet</code> object from the glmnet package?</p>\n",
      "\n",
      "<p>The <code>deviance.glmnet</code> function extracts the deviances, but I need the null (intercept-only) likelihood and the likelihood for the model with lambda that minimizes the cross-validated error.</p>\n",
      "\n",
      "<p>Here's my code for running glmnet – what's the next step?</p>\n",
      "\n",
      "<pre><code>&gt; fit_lasso &lt;- glmnet(x, y, alpha=1, family=\"poisson\", lambda.min.ratio=.001)\n",
      "&gt; fit_lasso_cv &lt;- cv.glmnet(x, y, alpha=1, family=\"poisson\", lambda.min.ratio=.001) \n",
      "</code></pre>\n",
      "\n",
      "*************************\n",
      "<p>I have a sequence of observed outcomes from independent but not identically distributed Bernoulli trials, and some covariate data. I perform logistic regression, and use it to estimate the probability of each trial succeeding. If I sum these estimates up, do they always add up to the total number of observed successes?</p>\n",
      "\n",
      "<p>I know that in the absence of covariate data, the MLE of the constant term is the number of successes divided by the number of trials.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for post in small_data[small_data.category == 'post'].text.sample(4).values:\n",
    "    print('*' * 25)\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the structure of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "Now we are discussing power and effect.I understand effect size and Cohen's d is one method to measure effect size.I struggled with understanding the power,type I and II errors,rejecting or accepting the null, and how these all relate to power and effect size. Why is an effect less meaningful if the sample is too large like on SAT or ACT tests males perform better in math than females, but the effect size is negligible due to the very large sample size..why? We are learning how to appraise research and studies.After this i must distinguish qualitative from quantitative studies ...any advice?\n",
      "*************************\n",
      "I don't understand the part that \"estimation method is biased\". From your description it seems you have a list of counts, so no estimation was done (so nothing can be biased). Maybe the sample is biased?\n",
      "*************************\n",
      "\"Most likely\" has to be the mode, since it has the highest probability mass.\n",
      "*************************\n",
      "Another good approach might be a bidirectional recurrent neural network -- used in machine translation tasks for *just* this purpose!\n"
     ]
    }
   ],
   "source": [
    "# sample of comments\n",
    "\n",
    "for sample in small_data[small_data.category == 'comment'].text.sample(4).values:\n",
    "    print('*' * 25)\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the raw text\n",
    "Were are going to remove the following characters:\n",
    "- html tags\n",
    "- line returns\n",
    "- URLs\n",
    "- Mathematical expressions in Latex\n",
    "- Integers numbers\n",
    "- @someone characters\n",
    "- digits\n",
    "- Language punctuations\n",
    "- and extra spaces\n",
    "\n",
    "For that we are using a series of simple regular expression patterns and the following pandas dataframe pattern:\n",
    "- $pattern = r\" some regex pattern\"$\n",
    "- $pd.DataFrame.text.apply(lambda t: re.sub(pattern, ' ', t)$\n",
    "\n",
    "Based on the specific domain knowledge and the task at end, it is up to us to decide which elements should be removed or kept. The sequence of transformation can be modified as well. Not also that the regular expression patterns used here are chosen for their simplicity. Feel free to use more precise patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"<[^>]*>\",' ', p))\n",
    "\n",
    "# remove line returns\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"[\\r\\n]+\",' ', p))\n",
    "\n",
    "# remove urls\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"http\\S+\",' ', p))\n",
    "\n",
    "# remove mentions: @someone\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"@\\S+\",' ', p))\n",
    "\n",
    "# remove latex expressions\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"\\$[^>]*\\$\",' ', p))\n",
    "\n",
    "# remove digit integers\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"\\d+\",' ', p))\n",
    "\n",
    "# remove some of the punctuation but keep ,.!? and - \n",
    "punctuation = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(punctuation)\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(pattern,' ', p))\n",
    "\n",
    "# remove multiple spaces\n",
    "small_data['text'] = small_data.text.apply(lambda p: re.sub(\"\\s\\s+\",' ', p))\n",
    "\n",
    "# remove trailling spaces with strip() function\n",
    "small_data['text'] = small_data.text.apply(lambda p: p.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Rule of thumb for excluded variable in Heckman selection model?\n",
      "-------------------------\n",
      "Accounting for complex survey design for structural equation modeling\n",
      "-------------------------\n",
      "How do I do an AB test with a binary response variable?\n",
      "-------------------------\n",
      "How much is moderate violation to normality for one sample t-test?\n"
     ]
    }
   ],
   "source": [
    "# titles should not be changed\n",
    "\n",
    "for p in small_data[small_data.category == 'title'].text.sample(4).values:\n",
    "    print('-' * 25)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "I'm trying to implement Lasso on high dimensional textual data. Format of Data p , , n , When running the Lasso, I get a training score of and the number of features selected as . This doesn't make sense to since the Lasso typically identifies a few sparse features. Now in order to check that I was running the model correctly, I also implemented an SVR and a Ridge Regression. The SVR worked just fine and the Ridge selected features. This leads me to think that my data is somehow too high dimensional for the Lasso. Is this correct or is there an alternative explanation for why the Lasso won't select any features and is returning a training score of . Sample of my code regressor lasso Lasso alpha , max iter e regressor lasso.fit X train,y train y pred regressor lasso.predict X test train score regressor lasso.score X train,y train test score regressor lasso.score X test,y test coeff used np.sum regressor lasso.coef ! Output - training score . - test score - . - number of features used Note that if I replace Lasso with Ridge, coeff used become and training score is no longer zero.\n",
      "*************************\n",
      "Error number of levels of each grouping factor must be lt number of observations This is a common error when using the lme package, and there are a number of questions about it on cross validated and elsewhere. The same error happens in different contexts. Here is a minimal example that illustrates my case. I am new to R, so probably there is a very obvous error. However at the moment I do not know what the error message means, so no idea how to correct it. The minimal example we would like to see if the mean height of humans category H , dogs D , and cats C is significantly different. We obtain the height of humans, dogs, and cats. In the actual experiment, these numbers would be much larger, and would be the same for each group. The R model is model lmer Height Category Id , data Data, REML TRUE where the Id reflects the fact that the particular humans or dogs, or cats chosen are a random sample from a population. Here is the code that produces the error library lme Category c H , H , D , D , D , C , C Id c human a , humanb , dog , dog , dog , cat , cat Height c ., ., ., ., ., ., . height in cm Data data.frame Category, Id, Height model lmer Height Category Id , data Data, REML TRUE anova model Error number of levels of each grouping factor must be lt number of observations Traceback . lmer Activation factor Category Image var , data Data, . REML TRUE Thank you for any guidance\n",
      "*************************\n",
      "First note you really should be also dividing by the standard deviation of each feature pixel value as well. Subtracting the mean centers the input to , and dividing by the standard deviation makes any scaled feature value the number of standard deviations away from the mean. To answer your question Consider how a neural network learns its weights. C NN s learn by continually adding gradient error vectors multiplied by a learning rate computed from backpropagation to various weight matrices throughout the network as training examples are passed through. The thing to notice here is the multiplied by a learning rate . If we didn't scale our input training vectors, the ranges of our distributions of feature values would likely be different for each feature, and thus the learning rate would cause corrections in each dimension that would differ proportionally speaking from one another. We might be over compensating a correction in one weight dimension while undercompensating in another. This is non-ideal as we might find ourselves in a oscillating unable to center onto a better maxima in cost weights space state or in a slow moving traveling too slow to get to a better maxima state. It is of course possible to have a per-weight learning rate, but it's yet more hyperparameters to introduce into an already complicated network that we'd also have to optimize to find. Generally learning rates are scalars. Thus we try to normalize images before using them as input into NN or any gradient based algorithm.\n",
      "*************************\n",
      "I think the problem might be in how the vcov calculates the variance-covariance matrix of the glmmTMB. It defaults to full FALSE, which doesn't return the full variance-covariance matrix. But now that it is running, I'm not sure it is returning sensible VIF values GVIF Df GVIF Df trtmt . . julian . . min aft sunrise . . snow cover . . trtmt julian . . Warning message In vif.default zip mod No intercept vifs may not be sensible.\n"
     ]
    }
   ],
   "source": [
    "# posts should have much less clutter\n",
    "\n",
    "for p in small_data[small_data.category == 'post'].text.sample(4).values:\n",
    "    print('*' * 25)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "Welcome to CV! I edited the title of your question to hopefully better reflect your question. What you are asking for is the essence of hypothesis testing and sample size estimation. I think the answer of is a good start. You may also want to read up on these two subjects a bit. As to the close votes This question may be very basic, but the purpose is clearly explained in the question. If someone has never been introduced to hypothesis testing, this is a very reasonable question.\n",
      "*************************\n",
      "Thanks for your answer but I am only transforming the independent variable and not the dependent ones so the above equation would not be valid here . Please correct me if I am wrong\n",
      "*************************\n",
      "here and here\n",
      "*************************\n",
      "For a specific x, you can do a goodness-of-fit test, such a Kolmogorov-Smirnov or Energy test. If you want to test for all x, you will run into the multiplicity multiple-testing problem. You cannot test for ALL x, but some finite set could be do-able.\n"
     ]
    }
   ],
   "source": [
    "# comments should also be less noisy\n",
    "\n",
    "for p in small_data[small_data.category == 'comment'].text.sample(4).values:\n",
    "    print('*' * 25)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the task of splitting a text into meaningful segments, called tokens. The input to the tokenizer is a unicde text, and the output is a container for accessing linguistic annotations for Doc. There many librairies to choose from. Here we use the Tokenization from spacy librairy. spaCy's tokenization is non-destructive, which means that you will always be able to reconstruct the original input from tokenized output. Whitespace information is preserved in the tokens and no information is added or removed during tokenization. This is kind of a core principle of spaCy's Doc object: \n",
    "- doc.text == input_text\n",
    "\n",
    "should always hold true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "small_data['tokens'] = small_data.text.apply(lambda token: tokenizer.tokenize(token.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many count of tokens do we have in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data['n_tokens'] = small_data.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    80000.000000\n",
       "mean        56.755738\n",
       "std         98.315008\n",
       "min          0.000000\n",
       "25%         15.000000\n",
       "50%         33.000000\n",
       "75%         66.000000\n",
       "max       7994.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc7d8c757d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_data.n_tokens.hist(bins = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I use a GLM to find the best fit for my included explanatory variables. I'm asked to estimate the effect size for this GLM and I can't find exactly what's about. I founnd this R code Recent version of R used . library 'pwr' library 'lmSupport' modelEffectSizes model modelPower u , v , alpha . , peta . How can I apply this for my dataset? In particular, I am unclear on what peta and u stand for. I suppose v refers to the sample size. Here is my dataset res structure list Motif structure c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , .Label c Home , Other , class factor , Type structure c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , .Label c Irregular , Regular , class factor , Times c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , Genre structure c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , .Label c Female , Male , class factor , Age c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , No c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , Yes c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , .Names c Motif , Type , Times , Genre , Age , No , Yes , row.names c NA, - L , class data.frame attach res model glm Yes Genre Times Type Age, family binomial summary model Edit Trial with effects package library 'effects' attach res mod.result lt - glm Yes Genre Times Type Age, family binomial eff.result lt - allEffects mod.result model Yes Genre Times Type Age Genre effect Genre Female Male . . Times effect Times . . . . . Type effect Type Irregular Regular . . Age effect Age . . . . . Is this the correct way to calculate effect size? Effects seems very low while I get a significant p-value for Genre and Times . Why is that? EDIT This is my last trial before losing hope on this library lmSupport attach res binom.mod glm Yes Genre Times Type Age, family binomial anova binom.mod modelEffectSizes binom.mod modelPower pc , pa , N , alpha . , peta . Results from Power Analysis pEta . pa pc alpha . N . What I understood is that I should take the lowest peta value from the ANOVA, that pc represent the number of predictors and pa the number of predictors together with the effect of interest. source\n"
     ]
    }
   ],
   "source": [
    "# this one has a very long series of \"L\"\n",
    "print(small_data[small_data.n_tokens > 5000].text.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the longest texts are composed of tables with limited semantic values. Let's remove rows that have more than an arbirtrary number of tokens (let's say 5000) as well rows that have too few tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77634, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data = small_data[(small_data.n_tokens > 4) & (small_data.n_tokens < 5000)].reset_index(drop=True)\n",
    "small_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    53115\n",
       "post       16434\n",
       "title       8085\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data\n",
    "We could export the dataframe as such using a pickle file format.\n",
    "But, if we want to keep the original csv format it is going to be easier, if we transform the list of tokens into a space separated string. On retrieval we will only have to split the string to get back the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i am analyzing data size survival of two group...\n",
       "1    does it mean that i am interpreting the nfl th...\n",
       "2    well , looking back on that , it is only true ...\n",
       "3    have you considered using the post-interventio...\n",
       "4    perhaps you 're already aware of this , but ch...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['tokens'] = small_data.tokens.apply(lambda token: ' '.join(token))\n",
    "small_data.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let us export the dataframe into a csv file. We will use that csv file as the new cleaned up and filtered out dataset to build our language model in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data.to_csv('stackexchange_small_data_tokenized.csv', quoting = csv.QUOTE_ALL, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Adding or removing steps to this first text processing taks will us to test different approaches in our language model building process. For example, we can decide not to remove the latex formatted mathematical expression and see if the language model is able to create grammatically valid equations. We could also implement a step to handle contractions (don't, let's, ...) and see if that improves the quality of the generated text. Finally, we could also decide to work on the vocabulary and filter out types or non-english unknown words using named entity recognition to tag specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
