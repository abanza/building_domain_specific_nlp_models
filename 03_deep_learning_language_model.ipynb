{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "POST_TYPE = 'post'\n",
    "MIN_TOKENS_LEN = 100\n",
    "MAX_TOKENS_LEN = 200\n",
    "DATA_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('/home/adelard/ml/manning/dataset/stackexchange_full_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_data[\n",
    "    (full_data.category == POST_TYPE) &\n",
    "    (full_data.n_tokens > MIN_TOKENS_LEN) &\n",
    "    (full_data.n_tokens < MAX_TOKENS_LEN)\n",
    "].sample(DATA_SAMPLE_COUNT).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape:  (20000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"data.shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"VEC model is a special case of VAR model. It exploits the time-series property of cointegration, i.e. that some linear combinations of unit-root time-series can be stationary. Because of this property VAR model has a special structure, which can be estimated via VECM. To apply VEC model at least two time series must be unit-root time series, because to define cointegration you need at least two unit-root time series. So the answers to your questions are No you cannot difference the non-stationary time series and then estimate VECM. Because estimating VECM on stationary time series does not make sense. Johansen's test can be applied only when all the series are non-stationary, because Johansen's test is for testing cointegration, and cointegration is defined only for unit root time series.\"\n",
      " \"As part of my thesis, I'm proving or attempting to prove... a few asymptotic results. Because these results depend on the condition number, I'd like to have some idea about the typical sizes of a condition numbers that crop up in social science research. That way, I can give some guidance about how large the sample size has to be before we reach the happy land of asymptopia. I'd be happy for any guidance. My very specific setup is as follows. For the standard Generalized Least Squares GLS model is the identity matrix would be most welcome!\"]\n"
     ]
    }
   ],
   "source": [
    "print(data.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "data['tokens'] = data.tokens.apply(lambda tok: np.array(tok.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['the', 'r', 'package', 'igraph', 'has', 'the', 'fit', 'power',\n",
      "       'law', 'function', 'which', ',', 'as', 'you', 'can', 'imagine',\n",
      "       ',', 'can', 'fit', 'a', 'power-law', 'to', 'a', 'vector.', 'as',\n",
      "       'you', 'can', 'see', 'in', 'the', 'reproducible', 'example',\n",
      "       'below', ',', 'one', 'of', 'the', 'outputs', 'of', 'this',\n",
      "       'function', 'is', 'the', 'log-likelihood', 'loglik', 'of', 'the',\n",
      "       'fitted', 'parameters.', 'question', 'is', 'there', 'a',\n",
      "       'rule-of-thumb', 'or', 'a', 'cut-off', 'value', 'that', 'tells',\n",
      "       'when', 'the', 'loglik', 'indicates', 'a', 'good', 'bad', 'fit',\n",
      "       '?', 'the', 'documentation', 'of', 'the', 'package', 'is',\n",
      "       'really', 'poor', 'in', 'explaining', 'how', 'this', 'parameter',\n",
      "       'should', 'be', 'considered', 'interpreted', 'reproducible',\n",
      "       'example', 'library', 'igraph', 'create', 'a', 'graph', 'set.seed',\n",
      "       'g', 'lt', '-', 'static.power.law.game', ',', ',', 'exponent.out',\n",
      "       '.', ',', 'exponent.in', '-', ',', 'loops', 'false', ',',\n",
      "       'multiple', 'true', ',', 'finite.size.correction', 'true', 'get',\n",
      "       'the', 'degree', 'distribution', 'like', 'this', 'd', 'lt', '-',\n",
      "       'degree', 'distribution', 'g', ',', 'mode', 'all', ',',\n",
      "       'cumulative', 't', 'd', 'lt', '-', 'd', 'd', 'gt', 'remove',\n",
      "       'unconnected', 'nodes', 'fit', 'power-law', 'fit', 'lt', '-',\n",
      "       'fit', 'power', 'law', 'd', ',', 'implementation', 'r.mle', 'fit',\n",
      "       'gt', 'ks.p', 'gt', '.'], dtype='<U22')]\n"
     ]
    }
   ],
   "source": [
    "print(data.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of tokens 2855851\n",
      "original vocab_size 53300\n",
      "nomber of tokens 2756723\n",
      "vocab_size 8701\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "# filter out words that are too scarce\n",
    "import itertools\n",
    "all_tokens = list(itertools.chain.from_iterable(data.tokens))\n",
    "\n",
    "# filter out least common tokens\n",
    "from collections import Counter\n",
    "counter_tokens = Counter(all_tokens)\n",
    "\n",
    "\n",
    "vocab_size  = len(set(all_tokens))\n",
    "vocab       = list(set(all_tokens))\n",
    "print(\"original number of tokens\", len(all_tokens))\n",
    "print(\"original vocab_size\", vocab_size)\n",
    "\n",
    "\n",
    "# remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "fltrd_tokens = [ token for token in all_tokens if counter_tokens[token] > TOKENS_MIN_COUNT ]\n",
    "\n",
    "print(\"nomber of tokens\", len(fltrd_tokens))\n",
    "print(\"vocab_size\", len(set(fltrd_tokens)))\n",
    "\n",
    "vocab_size  = len(set(fltrd_tokens))\n",
    "vocab       = list(set(fltrd_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered out tokens\n",
    "filtered_out_tokens = np.unique([token for token in all_tokens if counter_tokens[token] <= TOKENS_MIN_COUNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'-\" \"'-c\" \"'-category\" ... '𝜎' '𝜒²' '𝜔']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_out_tokens):  44599\n",
      "\n",
      "['honors' 'fitted.model' 'misinformed' 'xlm' 'philbin' 'dilip'\n",
      " 'x-haloperidol' 'quadrature' 'companions' 'mascara' 'desc' 'ab-c'\n",
      " 'mechanics—as' 'non-delinq' \"'banana\" 'y.m' 'pictured' 'administrator.'\n",
      " 'wpgmc' 'capitalised' 'lts' 'fe.' 'eoi' 'extinction.' 'nice.' 'clearly.'\n",
      " \"'g\" 'begs' 'derivatives.' '-mass' 'user-stratified' 'repetitively'\n",
      " 'sb.samp' 'sub-hourly' 'y-mx' 'roc.curve' 'tonnes' 'hyp.cov' 'tsclean'\n",
      " 'modelglm' 'thoma' 'axioms.' 'polson' 'dxdy' 'fitzmaurice' 'sot'\n",
      " 'concentrates' 'phyper' 'smoldyn' 'maarten' 'drifts' 'ret' 'bo'\n",
      " 'bi-modal' 'co-twin' 'infection.' 'intuitiveness.' 'xvals' 'logarthim'\n",
      " 'securities' 'incline' 'omiting' 'unmeaningful' 'dosages' 'neil' 'k-most'\n",
      " 'springer-verlag.' 'calories' 'fakedat' 'usv' 'reconvert'\n",
      " 'karhunen-loeve' 'bernd' 'gpy.models.gpregression' 'adj.mat' \"l'hospital\"\n",
      " 'earthquakes' 'motivations' 'warp' 'xttobit' 'awkward' 'constroptim'\n",
      " 'definitely.' 'photographs.' \"'pond\" 'sdv' 'variabels' 'anna' 'grater'\n",
      " 'stem-leaf' 'umm' 'kilobits.' 'actinomycetes' 'xtlogit' 'negating'\n",
      " \"'normalization\" 't-k' 'maxwell' 'log-scaled' \"selector'.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"len(filtered_out_tokens): \", len(filtered_out_tokens))\n",
    "print()\n",
    "print(np.random.choice(filtered_out_tokens, 100, replace = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('UNKNOWN')\n",
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens as vocabulary indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {x: i for i, x in enumerate(vocab)}\n",
    "\n",
    "def get_index(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNKNOWN']\n",
    "    \n",
    "data['tokens_index'] = data.tokens.apply(lambda tokens: np.array([get_index(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1131, 7836, 7013,  939, 8541, 8561, 8701,  502, 1279, 7609, 8506,\n",
      "       1131, 4799, 2640, 6600, 3843, 4604, 5940, 7459, 8701, 3720, 6600,\n",
      "       7609, 8541,  193,  502,  784, 1953, 3890, 3986,  502,  891, 3398,\n",
      "       2912, 3767, 8541,  193,  502, 5179, 8627, 1422, 5940,  502, 8541,\n",
      "       7756, 4604, 5940, 7609, 5744, 5727, 7888,  238, 8701, 6600, 3720,\n",
      "       8541, 2497,  290, 7426, 8701,  502, 1279, 5387, 1953, 7679, 8075,\n",
      "       6978, 7609, 3450, 5727, 1729, 7274, 3465,  223, 7609, 2912, 8541,\n",
      "       1015, 4783, 5744, 6600, 3537, 5860, 1696, 3720, 6164, 8541, 3100,\n",
      "       1141, 7609, 2596, 2070, 3361, 6979,   44, 7609, 2941, 4825, 4897,\n",
      "       7902, 6243, 6454, 3897, 1843, 7459, 3450, 2174])\n",
      " array([4709, 7429, 6600, 3720, 2354, 4611, 4609, 2160, 7110, 5516, 2452,\n",
      "       8541, 4801,  502, 8541, 3473, 2347, 5578, 7434, 3542, 1131, 5228,\n",
      "       1729, 3897, 2160, 8036, 2745, 7758, 3890, 2116, 4199, 2174, 4709,\n",
      "       3761, 6600, 6432, 4181, 2063,  151, 2347, 1131, 3675, 2640, 6600,\n",
      "       4478, 8701, 6780, 1417, 8537, 3005, 5521, 7609, 7609, 6185, 3562,\n",
      "       2000, 7609, 6077, 7101, 7609, 4738, 6198, 2174, 7609, 2174, 7609,\n",
      "       2174, 3797, 1417, 8537, 2000, 6780, 7609, 4153, 1417, 8537, 2000,\n",
      "       6780, 7609, 6631, 1417, 8537, 2000, 6780, 7609, 5514, 1417, 8537,\n",
      "       2972, 8701, 2174, 7609, 3302, 5825, 7609, 1141, 3797, 2326, 5514,\n",
      "       8701, 1417, 8537, 4663, 5514, 7609, 6631, 7609, 5696, 8701, 8160,\n",
      "       8701, 1417, 8537, 1257, 7609, 3562, 6631, 8701, 8701, 2907, 2174,\n",
      "       1417, 8537, 3890, 6600, 8541, 4401, 7046, 2907, 2157, 8701, 7609,\n",
      "       6631, 8701, 8701, 4277, 7002, 1131, 1146, 3720, 6836, 5387, 8209,\n",
      "       1131,  787, 1582, 1131,  105, 8541,  151, 6159, 8541, 5338, 4794,\n",
      "       3720,  162, 3773,  502, 4709, 4001, 1131, 8355, 2897, 3720,  583,\n",
      "       7347, 6649, 7459, 6368, 6244, 2068, 4523, 6600, 2160, 5747, 7274,\n",
      "       8104, 2174])\n",
      " array([3890, 8118, 6600, 2400, 1953, 2092, 7830, 7815, 8225, 2140,  169,\n",
      "       5940, 3822,  446, 7013, 4523, 2174, 8541, 6473, 6600,  415, 8293,\n",
      "       1870, 1953, 3738, 6600,  326, 4422, 6052, 5566, 3773, 7902,  502,\n",
      "       8541, 1141, 7308, 8541, 1400,  502, 8161, 3005, 2347, 4864, 8541,\n",
      "       3761, 3897, 8541, 4004, 1559,  966, 6947, 7758, 8541, 4001, 3365,\n",
      "       3450, 4066, 7902, 1141,  950, 7609, 3450, 7002, 4206, 2640, 7274,\n",
      "       5274, 7250, 3890, 6600, 8541, 2497,  770, 2941, 3450,  446, 1453,\n",
      "       4604, 6631, 8161,    0, 7642, 7609, 8541, 1360, 6600, 3720, 3450,\n",
      "       4608, 2912, 4066, 4604,  449, 7459, 7958, 1141, 4604, 8701, 4277,\n",
      "       3450, 4206, 8541, 7642, 7274, 7958, 8629, 6600, 4391, 4604, 3450,\n",
      "       7609, 2400, 7902, 7478, 6600, 1631, 6869, 7426, 8146, 7609, 7077,\n",
      "       5522, 8701, 4604, 2594, 6830,  502, 3890, 7478, 7902, 5816,  502,\n",
      "       4277,  716, 1141, 6600,  950, 3773, 2347, 4277,  716, 6600, 1713,\n",
      "       7459, 4885, 8541, 3761, 6600, 2565, 8541, 2146,  171, 7459, 4464,\n",
      "       8118, 5727, 6631, 2347, 3797, 5397, 2174])]\n"
     ]
    }
   ],
   "source": [
    "print(data.tokens_index.head(3).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generation(word):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(word) + SEQUENCE_WINDOW:\n",
    "        sequences.append(word[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "        \n",
    "    padded_seq = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sequence generation\n",
    "tokens_sequences = data.tokens_index.apply(sequence_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:46<00:00, 431.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences.shape:  (721510, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x = 0\n",
    "for seq in tqdm(tokens_sequences.values):\n",
    "    if x == 0:\n",
    "        all_seq = seq\n",
    "    else:\n",
    "        all_seq = np.concatenate((all_seq, seq))\n",
    "    x += 1\n",
    "    \n",
    "print(\"Sequences.shape: \", all_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences.shape:  (361110, 13)\n"
     ]
    }
   ],
   "source": [
    "# sample n% of the sequences to reduce the input dataset (optional)\n",
    "\n",
    "if True:\n",
    "    mask = np.random.choice([False, True], len(all_seq), p=[0.50, 0.50])\n",
    "    sequences = all_seq[mask].copy()\n",
    "else:\n",
    "    sequences = all_seq.copy()\n",
    "\n",
    "print(\"Sequences.shape: \", sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors.shape:  (361110, 12)\n",
      "Label.shape:  (361110,)\n",
      "Label_category.shape:  (361110, 8702)\n"
     ]
    }
   ],
   "source": [
    "# predictors and labels for the classification task\n",
    "\n",
    "predictors = sequences[:,:-1]\n",
    "label = sequences[:,-1]\n",
    "\n",
    "print(\"Predictors.shape: \", predictors.shape)\n",
    "print(\"Label.shape: \", label.shape)\n",
    "\n",
    "# keras to_categorical function transform the vocab_size vector of labels into a one hot encoded matrix\n",
    "# dimension (n, vocab_size)\n",
    "label_category = to_categorical(label, num_classes=vocab_size)\n",
    "print(\"Label_category.shape: \", label_category.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 64)            556928    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8702)              565630    \n",
      "=================================================================\n",
      "Total params: 1,270,782\n",
      "Trainable params: 1,270,782\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "dl_model = Sequential()\n",
    "dl_model.add(Embedding(vocab_size, embedding_dim, input_length=SEQUENCE_LEN -1))\n",
    "dl_model.add(LSTM(128, return_sequences=True))\n",
    "dl_model.add(LSTM(64))\n",
    "dl_model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "dl_model.compile(loss='categorical_crossentropy', \n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "print(dl_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1411/1411 [==============================] - 117s 83ms/step - loss: 5.7974 - accuracy: 0.1200\n",
      "Epoch 2/4\n",
      "1411/1411 [==============================] - 116s 82ms/step - loss: 5.5762 - accuracy: 0.1644\n",
      "Epoch 3/4\n",
      "1411/1411 [==============================] - 116s 82ms/step - loss: 5.5196 - accuracy: 0.1786\n",
      "Epoch 4/4\n",
      "1411/1411 [==============================] - 117s 83ms/step - loss: 5.4760 - accuracy: 0.1848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fadb788eed0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model fitting\n",
    "\n",
    "dl_model.fit(predictors, label_category, batch_size = 256, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample an index from a probability array\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def text_generation(nmax, text, temperature):\n",
    "    n = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    while (len(tokens) < nmax):\n",
    "        n += 1\n",
    "        \n",
    "        tokens_index = [vocab.index(word) if word in vocab else vocab.index('UNKNOWN') for word in tokens]\n",
    "        tokens_list = pad_sequences([tokens_index], maxlen=SEQUENCE_LEN-1, padding='pre')\n",
    "        probas = dl_model.predict(tokens_list, verbose=0)[0]\n",
    "        \n",
    "        next_word_index = sample(probas, temperature = temperature)\n",
    "        next_word = vocab[next_word_index]\n",
    "        \n",
    "        if next_word != '?':\n",
    "            print(next_word, probas[vocab.index(next_word)])\n",
    "            text += ' ' + next_word\n",
    "            \n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if n > 200:\n",
    "            break\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but 0.0014471592\n",
      "choice. 1.305962e-08\n",
      "tomorrow 5.7891114e-08\n",
      "dbscan 1.2885278e-08\n",
      "large 4.0152786e-06\n",
      "reject 6.6100747e-06\n",
      "diamonds 1.5589986e-09\n",
      "find 7.8479105e-05\n",
      "conditions 5.7608635e-05\n",
      "hyper-parameters 1.5598435e-07\n",
      "when 0.0050398842\n",
      "fill 4.516926e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a random variable but choice. tomorrow dbscan large reject diamonds find conditions hyper-parameters when fill'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation(15, 'a random variable', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_WINDOW = 1\n",
    "\n",
    "def perplexity_sentence(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(tokens)\n",
    "    \n",
    "    tokens_index = [vocab.index(word) if word in vocab else vocab.index('UNKNOWN') for word in tokens]\n",
    "    sequences = sequence_generation(tokens_index)\n",
    "    predictors = sequences[:,:-1]\n",
    "    label = sequences[:,-1]\n",
    "    probas = dl_model.predict(predictors, verbose=0)\n",
    "    logprob = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        p = probas[i, label[i]]\n",
    "        logprob += np.log(p)\n",
    "    return np.exp(-logprob / N), logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy for any guidance. My very specific setup is as follows. (301.3829084343486, -85.62572365999222)\n"
     ]
    }
   ],
   "source": [
    "# sentence comparison - from the corpus\n",
    "sentence = \"I'd be happy for any guidance. My very specific setup is as follows.\"\n",
    "print(sentence, perplexity_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They tell me that my heart misses you. (1314.5522166093801, -64.63126230239868)\n"
     ]
    }
   ],
   "source": [
    "# sentence comparison - from non-corpus\n",
    "sentence = \"They tell me that my heart misses you.\"\n",
    "print(sentence, perplexity_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the standard Generalized Least Squares GLS model is the identity matrix would be most welcome! (701.0383460059271, -111.39356398582458)\n"
     ]
    }
   ],
   "source": [
    "# sentence comparison - from the corpus\n",
    "sentence = \"For the standard Generalized Least Squares GLS model is the identity matrix would be most welcome!\"\n",
    "print(sentence, perplexity_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity on a validation set\n",
    "validation_data = full_data[(full_data.category == 'title') & \n",
    "                            (full_data.n_tokens > 10)].sample(100, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data (100, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"validation_data\", validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data sample:     post_id  parent_id  comment_id  \\\n",
      "0    37944        NaN         NaN   \n",
      "1    93935        NaN         NaN   \n",
      "2   165022        NaN         NaN   \n",
      "\n",
      "                                                text category  \\\n",
      "0  What are easy to interpret, goodness of fit me...    title   \n",
      "1  How to make stochastic gradient descent algori...    title   \n",
      "2  Expectation of a random variable and the indic...    title   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  what are easy to interpret , goodness of fit m...        16  \n",
      "1  how to make stochastic gradient descent algori...        12  \n",
      "2  expectation of a random variable and the indic...        11  \n"
     ]
    }
   ],
   "source": [
    "print(\"validation_data sample: \", validation_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_corpus(corpus):\n",
    "    corpus_sentences = ''.join(corpus)\n",
    "    corpus_tokens = tokenizer.tokenize(corpus_sentences.lower())\n",
    "    N = len(corpus_tokens)\n",
    "    logproba = 0\n",
    "    perplex_result = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        perplex, logpro = perplexity_sentence(sentence)\n",
    "        logproba += logpro\n",
    "        perplex_result.append(perplex)\n",
    "        \n",
    "        print(\"{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\".format(\n",
    "            perplex, np.mean(perplex_result), logpro, logproba, np.exp(-logproba / (N)), sentence))\n",
    "        \n",
    "    return np.exp(-logproba / (N)), perplex_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.97\t222.97\t-86.51\t-86.51\t1.07\twhat are easy to interpret , goodness of fit measures for linear mixed effects models ?\n",
      "412.05\t317.51\t-72.25\t-158.77\t1.12\thow to make stochastic gradient descent algorithm converge to the optimum ?\n",
      "797.01\t477.34\t-73.49\t-232.26\t1.19\texpectation of a random variable and the indicator random variable proof\n",
      "1589.74\t755.44\t-95.83\t-328.08\t1.27\tneural network gives exact same values of previous day for day ahead prediction\n",
      "141.49\t632.65\t-64.38\t-392.46\t1.33\tcan someone explain the concept of ancillary statistics in layman 's terms ?\n",
      "1283.29\t741.09\t-93.04\t-485.51\t1.43\twhy are n't gaussian activation functions used more often in neural networks ?\n",
      "314.63\t680.17\t-63.27\t-548.77\t1.49\tgiven a frequency distribution , calculate probability of a given b\n",
      "5877.15\t1329.79\t-112.82\t-661.60\t1.62\tstata demean and detrend after dickey-fuller do not reduce p-value to insignificant levels\n",
      "1025.92\t1296.03\t-83.20\t-744.80\t1.73\tcontrolling for overall economic factors and seasonality in a time series ?\n",
      "81.48\t1174.57\t-66.01\t-810.80\t1.81\tcan one sample t test be used to check statistical significance for my data ?\n",
      "32.88\t1070.78\t-45.41\t-856.21\t1.87\ti need to know which type of anova to use for this problem\n",
      "3070.53\t1237.43\t-104.38\t-960.59\t2.02\tkendall 's coefficient of concordance w for ratings with a lot of ties\n",
      "1980.53\t1294.59\t-106.28\t-1066.87\t2.18\twhich papers discuss classification or clustering of source code according to programming language ?\n",
      "212.93\t1217.33\t-91.14\t-1158.01\t2.33\tis the size of the sum of squared errors in ols regression invariant to linear transformation ?\n",
      "1267.95\t1220.70\t-92.89\t-1250.89\t2.50\tdoes n't the non-gaussian source assumption of ica render it practically useless ?\n",
      "2448.93\t1297.47\t-132.66\t-1383.55\t2.75\twould you accept this normality for anova ? asked yesterday , but data revised and different graph\n",
      "292.15\t1238.33\t-68.13\t-1451.68\t2.89\twhich test to use for paired , nonparametric , categorical data ?\n",
      "188.35\t1180.00\t-110.00\t-1561.68\t3.14\ttraining test data with time series model -- forecast with training model , or with model based on full data ?\n",
      "341.38\t1135.86\t-93.33\t-1655.01\t3.36\thow to measure argue the goodness of fit of a trendline to a power law ?\n",
      "3355.98\t1246.87\t-105.54\t-1760.55\t3.63\tpattern recognition not possible with linear classifiers for local and distributed information ?\n",
      "244.84\t1199.15\t-60.51\t-1821.06\t3.79\thow to decide the numbers of row column clusters for co-clustering\n",
      "2146.70\t1242.22\t-84.39\t-1905.45\t4.03\thow to project high dimensional space into a two-dimensional plane ?\n",
      "1090.05\t1235.61\t-83.93\t-1989.37\t4.29\thypothesis testing if two random variables come from a related underlying function\n",
      "21.81\t1185.03\t-33.91\t-2023.28\t4.40\thow do i combine the standard deviations of these distributions ?\n",
      "549.07\t1159.59\t-82.01\t-2105.29\t4.67\thow well should features discriminate to build a good classifier from them ?\n",
      "324.71\t1127.48\t-63.61\t-2168.90\t4.89\tchi-squared to test if two variables have the same frequency distribution\n",
      "762.35\t1113.96\t-99.55\t-2268.45\t5.26\thow to statistically make inferences about how close are repeated events to each other ?\n",
      "464.47\t1090.76\t-79.83\t-2348.28\t5.58\tnested anova , three way anova , mixed model , or ancova ?\n",
      "280.41\t1062.82\t-62.00\t-2410.28\t5.84\thow to add confidence bands to a nonlinear regression model ?\n",
      "366.58\t1039.61\t-64.95\t-2475.22\t6.12\twhy is there a e in the name em algorithm ?\n",
      "732.26\t1029.70\t-72.56\t-2547.78\t6.46\thow do you estimate long-run coefficients from ardl bounds test ?\n",
      "143.28\t1002.00\t-54.61\t-2602.39\t6.72\twhat is the formal way to do subtraction on probabilities ?\n",
      "4611.49\t1111.38\t-92.80\t-2695.19\t7.19\twhy is sufficient statistics data reduction normally taught in statistics ?\n",
      "147.91\t1083.04\t-54.96\t-2750.16\t7.49\tdo we need a test set when using k-fold cross-validation ?\n",
      "70.23\t1054.10\t-68.03\t-2818.18\t7.87\tcross-validation what is the standard deviation if the same value is obtained for each fold ?\n",
      "80.60\t1027.06\t-57.06\t-2875.25\t8.21\thow do we know that a model really has a predictive power ?\n",
      "33.66\t1000.21\t-38.68\t-2913.93\t8.44\thow to determine the type of regression to be used ?\n",
      "1328.42\t1008.85\t-115.07\t-3028.99\t9.18\twhat among location , scale and shape is kolmogorov–smirnov test statistic sensitive to and why ?\n",
      "160226.26\t5091.35\t-143.81\t-3172.81\t10.20\tspare part inventory management calculating safety stock for non-normally distributed demand ?\n",
      "324.21\t4972.17\t-86.72\t-3259.53\t10.87\thow to incorporate uncertainty and noise information in training and prediction of neural networks ?\n",
      "383.70\t4860.25\t-71.40\t-3330.93\t11.46\thow to test whether e x e y controlling for z ?\n",
      "753.17\t4762.47\t-79.49\t-3410.42\t12.14\tstatistical test for the difference in center positions of two receptive fields\n",
      "432.95\t4661.78\t-72.85\t-3483.27\t12.81\ttransform moments of a random variable to fit the moments of another\n",
      "142.52\t4559.07\t-84.31\t-3567.58\t13.62\twhat is the name for the distribution shape of a histogram with this kind of curvature ?\n",
      "1213.50\t4484.72\t-85.22\t-3652.79\t14.50\tdoes significance test make sense to compare randomised groups at baseline ?\n",
      "653.90\t4401.44\t-71.31\t-3724.10\t15.28\twhy is the noncentral chi-squared approximate of the deviance bad ?\n",
      "1533.75\t4340.43\t-110.03\t-3834.14\t16.56\tposterior mode , posterior mean and posterior variance of a posterior distribution of dirichlet form\n",
      "320.57\t4256.68\t-63.47\t-3897.61\t17.34\tare there any fast approximations to generalized linear mixed models ?\n",
      "795.02\t4186.04\t-106.85\t-4004.46\t18.76\ttime series forecasting in r for a period less than years months which is totally random\n",
      "675.82\t4115.83\t-97.74\t-4102.20\t20.15\tdoes covariance matrix of a rotated data have the information of angle of rotation ?\n",
      "2121.56\t4076.73\t-91.92\t-4194.12\t21.55\tgeneral solution sum of two uniform random variables ay bx z ?\n",
      "3792.38\t4071.26\t-98.89\t-4293.01\t23.17\tnot convinced about normality based on qq plot , shapiro and ks\n",
      "404.22\t4002.07\t-78.03\t-4371.03\t24.53\twhy are my neural network predictions so wrong when i add another variable\n",
      "488.61\t3937.01\t-68.11\t-4439.14\t25.78\thow likely are various outcomes in a lottery with multiple prizes\n",
      "60.54\t3866.53\t-69.76\t-4508.90\t27.13\thow do i transform my data so that it has mean zero and standard deviation one ?\n",
      "47.47\t3798.33\t-61.76\t-4570.66\t28.39\tis there any kind of svm or other linear models can be applied to such scenery？\n",
      "372.00\t3738.22\t-88.78\t-4659.44\t30.30\tstatistical test for difference in mean of means non-normally distributed , different number of cases\n",
      "386.67\t3680.43\t-89.36\t-4748.81\t32.34\tmeasuring correlation when multiple data points in a sample are provided by the same subject\n",
      "144.85\t3620.51\t-59.71\t-4808.52\t33.79\thow do i define an interaction contrast with single explantory variable ?\n",
      "63.71\t3561.23\t-58.16\t-4866.68\t35.26\twhat is the appropriate analysis for this type of repeated measures multi-binary data ?\n",
      "557.81\t3511.99\t-113.83\t-4980.51\t38.32\twhy is inverted gamma , . a diffuse prior when variance of this random variable is small ?\n",
      "24.45\t3455.74\t-35.16\t-5015.67\t39.32\thow can i interpret what i get out of pca ?\n",
      "8977.66\t3543.39\t-163.84\t-5179.51\t44.33\ttesting difference between two means. skewed n vs symmetric bell n . wilcoxon rank sum test appropriate ?\n",
      "210.24\t3491.31\t-112.31\t-5291.83\t48.13\thow to deal with a systematic bias in the random forest model , and what are possible alternative modeling approaches ?\n",
      "714.82\t3448.59\t-72.29\t-5364.12\t50.75\tasa discusses limitations of -values - what are the alternatives ?\n",
      "1667.18\t3421.60\t-96.45\t-5460.57\t54.46\tnewbie wondering about standard deviation and how to detect outliers in new datapoints\n",
      "264.82\t3374.49\t-111.58\t-5572.15\t59.10\thow do i know what my vif limits should be for collinearity should be when doing binary logistic regression ?\n",
      "2838.53\t3366.60\t-95.41\t-5667.56\t63.37\tis this analysis prudent ? running several multiple regressions to gain insights\n",
      "1002.08\t3332.34\t-96.74\t-5764.30\t68.02\tin word vec , for analogies do we use in or out vectors ?\n",
      "106.87\t3286.26\t-70.07\t-5834.37\t71.60\thow to approach time series regression with one continuous variable and one logistical variable ?\n",
      "183.12\t3242.55\t-98.99\t-5933.36\t76.98\thow to find whether the time series is seasonal and to find the period of seasonality if seasonal ?\n",
      "177.79\t3199.99\t-72.53\t-6005.89\t81.18\tis there a way to disable the parameter tuning grid feature in caret ?\n",
      "693.49\t3165.65\t-104.67\t-6110.56\t87.65\tdo i discuss the lack of interaction if i already discussed lack of main effects ?\n",
      "366.41\t3127.82\t-94.46\t-6205.02\t93.92\twhy is there no assumptions about ivs ' and dv 's distribution in multiple linear regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010.34\t3099.59\t-131.44\t-6336.46\t103.41\twhat is the rationale behind lars-ols hybrid , i.e. using ols estimate on the variables chosen by lars ?\n",
      "2121.66\t3086.72\t-107.24\t-6443.70\t111.86\twhy create a generator manually for neural network when keras has built-in generators ?\n",
      "703.33\t3055.77\t-85.23\t-6528.93\t119.06\tdealing with hierarchical panel , multi-level data and fixed effects in lasso ?\n",
      "10920.28\t3156.60\t-111.58\t-6640.51\t129.19\tfitting probability distribution to failure data with discrete , right censored stress\n",
      "797.43\t3126.73\t-80.18\t-6720.68\t137.00\ttesting large dataset for normality - how and is it reliable ?\n",
      "16.96\t3087.86\t-36.80\t-6757.49\t140.74\twhat is the distribution of the sample variance of the skellam distribution ?\n",
      "214.27\t3052.38\t-59.04\t-6816.53\t146.96\tmeta-analysis how to find the mean age of all studies ?\n",
      "4669.35\t3072.10\t-109.83\t-6926.36\t159.26\trnorm gives me x coordinate or y coordinate of normal distribution graph ?\n",
      "240.21\t3037.98\t-82.22\t-7008.58\t169.14\thow to answer a clients question on how accurate your logistic regression model is ?\n",
      "594.13\t3008.89\t-108.58\t-7117.16\t183.13\tsuppose are i.i.d. random variables. when is the sequence expected to decrease for the first time ?\n",
      "182.26\t2975.64\t-119.72\t-7236.89\t199.91\twhat heterogeneity test is appropriate for my study ? and , what is the use of correlation between median pfs and median os\n",
      "990.00\t2952.55\t-75.87\t-7312.76\t211.33\tbarnard 's exact test boschloo 's variant when distribution is hypergeometric\n",
      "331.14\t2922.42\t-81.24\t-7394.00\t224.28\thow to cut features with large amount of values from high dimensional data ?\n",
      "1092.38\t2901.62\t-83.95\t-7477.95\t238.49\ttesting -way interaction of categorical variables in a meta analysis in r\n",
      "5880.69\t2935.09\t-138.87\t-7616.82\t264.01\twhy are samples within a cluster less informative than randomly chosen ones from entire population ?\n",
      "1024.10\t2913.86\t-103.97\t-7720.80\t284.89\tpoisson distribution and completeness , what happens when one point removed from parameter space ?\n",
      "973.22\t2892.53\t-96.33\t-7817.12\t305.71\texample for uncorrelated , not independent , but with same distribution functions random variables\n",
      "140.71\t2862.62\t-89.04\t-7906.17\t326.30\troll dice times , what is the probability that the total number of dots is at least ?\n",
      "972.75\t2842.30\t-75.68\t-7981.85\t344.89\tdifference between regression between groups vs across all subjects continuum ?\n",
      "92.24\t2813.05\t-49.77\t-8031.62\t357.69\tis it possible to compare multiple proportions in subdata sets ?\n",
      "316.20\t2786.76\t-63.32\t-8094.94\t374.66\twhy can the null hypothesis be something other than equality ?\n",
      "348.01\t2761.36\t-99.49\t-8194.42\t402.96\thow to control for a numeric variable when doing a chi square goodness of fit test ?\n",
      "623.34\t2739.32\t-135.14\t-8329.56\t444.87\twhat are some software packages for doing panel aggregated logistic regression with no random or fixed effect intercept not sas ?\n",
      "1148.79\t2723.09\t-112.74\t-8442.30\t483.14\twhat to conclude about these models ? random intercept fixed slope vs. random intercept and slope\n",
      "236.36\t2697.97\t-60.12\t-8502.42\t504.88\twhat are some models that performs like linear discriminant analysis ?\n",
      "166.20\t2672.65\t-117.60\t-8620.03\t550.27\thow can i test for a systematic increase in y as a function of x in data modeled by a sigmoid curve ?\n",
      "Perplexity corpus: 550.27\n"
     ]
    }
   ],
   "source": [
    "corpus = validation_data.tokens.values\n",
    "perplexity_score, scores = perplexity_corpus(corpus)\n",
    "print(\"Perplexity corpus: {:.2f}\".format(perplexity_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeM0lEQVR4nO3deZwcVb338c+XEPYAhgEMDDIQcQFFhAji8ogXVNaL1wVFdtCIjwte8bkieAVFvLih4oZBliCIKItwcUFEIYggEIgsBoRAWCM8gzAZ4KIk/O4f57RUOjM9NZ2pnqW+79erX1N1avvV6elfV5+qOqWIwMzM6mOl0Q7AzMw6y4nfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4xxlJO0l6cATWc4qk/xyJmCYKSRtKmiOpX9LXRjue8UhSj6SQtPIg04+W9INOx2XLGvDNsYkvIg5vDEvaCTg7IrpHL6IxYSbQC6wdHbrBpW51HxFfLDOfpCtJ9eIviQr4iH8MGewoqe46WC+bAn/uVNK30eHPGRARfq3gC1gIfBr4M/A4cAawWmH6nsA84AngD8DWTct+CrgF+DvpV9ig6wN2Ah4sLL8RcAHw/4F7gY/l8qnAg8BeeXwt4G7gwDx+JvAFYE3gf4DngCfzayPgaWC9wna2y9uYPMD+bw/cCCwGHgFOKkx7Q97nJ4AHgINz+TrAWXmd9wGfAVbK0w4GrgG+Dvwtx7kq8FXg/ryNU4DV8/xdwKV5G38Drm6sa4BYXwfcAPTlv68r1MezwD9yHewywLK75/ekH3gI+OQw3uNP5ve4DzgPWK1F3a8EHAUsAB4DfgJMzevqAQI4KNdFL3BMYVuTgKPzsv3AXGCTPO1lwOW5ju4E9imzb8P8LAwV33GkI3lyHZyd9/GJ/H5sCJwALAWeyXXy7VbvXZ62GTAnx/8b4DuF7TRiOizHNCeX/xT4a17fHGCrwvrOBL4L/DLHcA3wQuAbpM/kHcCrRzv3tJ2zRjuAifDKH+zbgE1ICfca4At52rbAo8AO+UN5UJ5/1cKy8/Kyq5dY307kxE9KEHOBzwKrAJsD9wBvy9Pfmv+xNwBOBc4vxHzmQOssTP8F8KHC+NeBbw2y/9cCB+ThtYDX5uEX5Q/ivsBkYD1gmzztLOBiYEr+YP4FOCxPOxhYAnyU9EW4ev7AXZLrYwrw38B/5fn/i/RFMDm/3ghogDin5g/tAXm9++bx9ZrrZJD9XAS8MQ+/ANh2GO/x9aSkPhWYDxzeou4/DlwHdJO+8L4PnJun9ZCS2Km5Xl5FOmB4eZ7+/4BbgZcCytPXI33JPAAckvd9W1JS3qrVvrXxWRgqvuN4PiF/ML+Pa+R6247UzAZwJfD+Ybx315IODFYhHWwsZvnEf1auh8bn7FDS/9KqpP+veU2fj94c02rAb0kHVgfmWL8A/G60c0/bOWu0A5gIr/zBPrwwvjuwIA9/Dzi+af47gTcVlj10GOv7Z6IgJZr7m5b9NHBGYfxbORE8zLJH8GfSOvG/B7gmD08ifYFsP8j+zwE+B3QNEMtFA8w/KSeDLQtlHwSuzMMHF/eLlMCeAqYXynYE7s3Dnyd9ibx4iPfpAOD6prJref5XyD/rZJDl789xrt1UXuY93r8w7cvAKS3qfj6wc2F8GunXyMo8n8S6C9OvB95b2O7eA8T+HuDqprLvA8e22rc2PgtDxXcczyfkQ2n6dVRY5kqWTfyDvnekA4wlwBqFaWezfOLfvEXc6+Z51in8L5xamP5RYH5h/JXAEytSV6P5chv/yHmgMHwf6egOUrvxkZKeaLxIR/IbDbLsUOsr2hTYqGndR5N+LjfMAl5B+jJ4bBj7czGwpaTNgbcAfRFx/SDzHga8BLhD0g2S9szlm5CaHJp1kY7M7iuU3QdsXBgv7v/6pKPCuYX9/FUuB/gKqRnr15LukXTUIHFu1LTNgbbbyjtJX8L3SbpK0o65vMx7/NfC8NOkX0aD2RS4qLCu+aSmj+L7Otj6BqvzTYEdmmLcj9R80WrfliHpdklP5tcbW+xDmf39IXAZ8GNJD0v6sqTJg6yv1Xu3EfC3iHi6MK3lZ0rSJEknSlogaTHpyxnS/2bDI4Xh/xlgvNV7OKY58Y+cTQrDLyIdYUP6ZzshItYtvNaIiHML88cw1lf0AOmot7juKRGxO6R/btJR3VnAhyS9eJDYl9t+RDxDalvej3S09cNBliUi7oqIfUlNSl8CzpfUaFqYPsAivaQj2E2b9vGhQWLqJX3Qtirs5zoRsVbefn9EHBkRmwN7AZ+QtPMA2324aZsDbXdQEXFDROyd9/NnpPqBcu/xoKsdoOwBYLem9a0WEWXiHKzOHwCualrnWhHxoSH2bdlgI7bKy60VEVeXiGdQEfFsRHwuIrYktd/vSWpKgeXrpdV7twiYKmmNwrRNWF5xne8D9gZ2IZ1v6snlGuZujEtO/CPnw5K6JU0lHXWfl8tPBQ6XtIOSNSXtIWlKm+sruh5YLOlTklbPRzGvkPSaPP3o/PdQUvvnWfnLoNkjwHqS1mkqP4v0U/pfST+dByRpf0nrR8RzpJN0kI5QzwF2kbSPpJUlrSdpm4hYSkosJ0iaImlT4BODbSOv91Tg65I2yNvcWNLb8vCekl4sSaS23aX51ewXwEskvS/H8x5gS9KJ4ZYkrSJpP0nrRMSzhe1A++8xDFz3p5DqZtO87fUl7V1iXQA/AI6XtEWOZWtJ6+V9fImkAyRNzq/XSHr5EPtWGUlvlvTK/D+5mHQw0NjuI6RzVg2DvncRcR/p4oLj8r7sSDoAaGUKqbnxMdKvyVKXmU4UTvwj50fAr0knV+8hnfwhIm4EPgB8m3Qy6m5SMm1rfUU5ge4FbEM68dRL+uCvI2k7UjI9MM/3JdIRz3LNIBFxB3AucE9uBtgol19DuuLkpohY2CLWXYHbJT0JfJPUnvtMRNxPaj44knQlyTzSyT5IbaZP5X37fd7f01ts41Okursu/zT/DekEJsAWefxJUrvvdyPiygH28zHSUeWRpA/8fwB7RkRvi+0WHQAszNs/HNg/r7fd93iwuv8m6UT2ryX1k0707lAyxpNIX6q/JiXT00gnM/tJJ/vfSzp6/ivpf2LVVvtWsRcC5+c45wNX8fyX/zeBd0l6XNLJJd67/UjnfR4jfVbOIyX2wZxFaip6iHQ103UjuF9jnvKJClsBkhaSTkT9ZiyubwVj+S3wo/CNNDaOSDoPuCMijh3tWMYiH/HboHKT0bYM3MxkNmbkZqvpklaStCup/f5nox3XWOU72GxAkmYDbweOyM0EZmPZC4ELSfcsPEi6B+Xm0Q1p7HJTj5lZzVTW1CNpE0m/kzQ/X/t7RC4/TtJDkubl1+5VxWBmZsur7Ihf0jRgWkTclC9rm0tqOtgHeDIivlp2XV1dXdHT0zPsGJYsWcLKK9e7Nct1kLgeXAcNdaqHuXPn9kbE+s3lle19RCwi3VhBRPRLmk/5OySX0dPTw4033jjs5Xp7e+nq6hp6xgnMdZC4HlwHDXWqB0nNdzsDHTq5K6kHeDXwR+D1wEckHUi66eLIiHh8gGVmkvpHp7u7m97espdaP6+vr6/9oCcI10HienAdNLgeOpD4Ja1F6jb44xGxWNL3gONJNxMdD3yNdGfpMiJiFqmfGWbMmBHtfkPX5Zu9FddB4npwHTTUvR4qvY4/d7h0AXBORFwIEBGPRMTSwm3421cZg5mZLavKq3pEul18fkScVCifVpjt30j9zpuZWYdU2dTzelL/H7dKmpfLjgb2lbQNqalnIakPcDMz65Aqr+r5PQN3cfqLqrZpZmZDc189ZmY148RvZlYzTvxmZjUz4e9b7jnq58Oaf+GJe1QUiZnZ2OAjfjOzmnHiNzOrGSd+M7OaceI3M6sZJ34zs5px4jczqxknfjOzmpnw1/EP13Cv+wdf+29m44uP+M3MasaJ38ysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqxonfzKxmKkv8kjaR9DtJ8yXdLumIXD5V0uWS7sp/X1BVDGZmtrwqj/iXAEdGxMuB1wIflrQlcBRwRURsAVyRx83MrEMqS/wRsSgibsrD/cB8YGNgb2B2nm028PaqYjAzs+Wt3ImNSOoBXg38EdgwIhZB+nKQtMEgy8wEZgJ0d3fT29s77O329fUxfUq0GXV57cTWKX19faMdwpjgenAdNLgeOpD4Ja0FXAB8PCIWSyq1XETMAmYBzJgxI7q6utra/oL+cttbEe3G1iljPb5OcT24DhrqXg+VXtUjaTIp6Z8TERfm4kckTcvTpwGPVhmDmZktq8qregScBsyPiJMKky4BDsrDBwEXVxWDmZktr8qmntcDBwC3SpqXy44GTgR+Iukw4H7g3RXGYGZmTSpL/BHxe2CwBvadq9qumZm15jt3zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqZliJX9ILJG1dVTBmZla9IRO/pCslrS1pKvAn4AxJJw21nJmZjU1ljvjXiYjFwDuAMyJiO2CXasMyM7OqlEn8K+deNPcBLq04HjMzq1iZxP954DJgQUTcIGlz4K5qwzIzs6oM2UlbRPwU+Glh/B7gnVUGZWZm1Slzcvclkq6QdFse31rSZ6oPzczMqlCmqedU4NPAswARcQvw3iqDMjOz6pRJ/GtExPVNZUuqCMbMzKpXJvH3SpoOBICkdwGLKo3KzMwqU+YJXB8GZgEvk/QQcC+wf6VRmZlZZcpc1XMPsIukNYGVIqK/+rDMzKwqZa7q+aKkdSPiqYjoz/31fKETwZmZ2cgr08a/W0Q80RiJiMeB3asLyczMqlQm8U+StGpjRNLqwKot5jczszGszMnds4ErJJ1BurLnUGB2pVGZmVllypzc/bKkW4GdAQHHR8RllUdmZmaVKHPET0T8EvhlxbGYmVkHlLmq5x2S7pLUJ2mxpH5JizsRnJmZjbwyR/xfBvaKiPlVB2NmZtUrc1XPI076ZmYTR5kj/hslnQf8DPh7ozAiLqwsKjMzq0yZxL828DTw1kJZAE78ZmbjUJnLOQ/pRCBmZtYZfgKXmVnN+AlcZmY14ydwmZnVTGVP4JJ0uqRHG01Euew4SQ9Jmpdf7uXTzKzD2n0C134lljsT+DZwVlP51yPiq8MJ0szMRk6ZxB8RscwTuCRtVmKhOZJ6VjRAMzMbWWUS/wXAthHxVKHsfGC7Nrf5EUkHAjcCR+YHuyxH0kxgJkB3dze9vb3D3lBfXx/Tp0SbYZbXTmyd0tfXN9ohjAmuB9dBg+uhReKX9DJgK2AdSe8oTFobWK3N7X0POJ50vuB44Guk/v2XExGzSE1MzJgxI7q6utra4IJ+tbXccLQbW6eM9fg6xfXgOmioez20OuJ/KbAnsC6wV6G8H/hAOxuLiEcaw5JOBS5tZz1mZta+QRN/RFwMXCxpx4i4diQ2JmlaRDSuCPo34LZW85uZ2cgr08Z/t6SjgZ7i/BExYBNNg6RzgZ2ALkkPAscCO0nahtTUsxD4YFtRm5lZ28ok/ouBq4HfAEvLrjgi9h2g+LSyy5uZWTXKJP41IuJTlUdiZmYdUebO3Ut9h62Z2cRRJvEfQUr+z/iZu2Zm41+Z/vindCIQMzPrjDL98UvS/pL+M49vImn76kMzM7MqlGnq+S6wI/C+PP4k8J3KIjIzs0qVuapnh4jYVtLNABHxuKRVKo7LzMwqUuaI/1lJk3i+P/71gecqjcrMzCpTJvGfDFwEbCDpBOD3wBcrjcrMzCpT5qqecyTNBXYGBLw9IuZXHpmZmVWizFU904F7I+I7pE7V3iJp3cojMzOzSpRp6rkAWCrpxcAPgM2AH1UalZmZVaZM4n8uIpYA7wC+GRH/DkyrNiwzM6tK2at69gUO5PkHp0yuLiQzM6tSmcR/COkGrhMi4t78oPWzqw3LzMyqUuaqnj8DHyuM3wucWGVQZmZWnTJH/GZmNoE48ZuZ1cygiV/SD/PfIzoXjpmZVa3VEf92kjYFDpX0AklTi69OBWhmZiOr1cndU4BfAZsDc0ndNTRELjczs3Fm0CP+iDg5Il4OnB4Rm0fEZoWXk76Z2ThV5nLOD0l6FfDGXDQnIm6pNiwzM6tKmU7aPgacA2yQX+dI+mjVgZmZWTXKPIHr/aSncD0FIOlLwLXAt6oMzMzMqlHmOn4BSwvjS1n2RK+ZmY0jZY74zwD+KOmiPP524LTqQjIzsyqVObl7kqQrgTeQjvQPiYibqw7MzMyqUeaIn4i4Cbip4ljMzKwD3FePmVnNOPGbmdVMy8QvaZKk33QqGDMzq17LxB8RS4GnJa3ToXjMzKxiZU7uPgPcKuly4KlGYUR8bPBFzMxsrCqT+H+eX8Mi6XRgT+DRiHhFLpsKnAf0AAuBfSLi8eGu28zM2jfkyd2ImA38BLguImY3XiXWfSawa1PZUcAVEbEFcEUeNzOzDirTSdtewDxS3/xI2kbSJUMtFxFzgL81Fe8NNL40ZpPuAjYzsw4qcznnccD2wBMAETEP2KzN7W0YEYvyehaRevs0M7MOKtPGvyQi+qRl+mWLiuL5J0kzgZkA3d3d9Pb2DnsdfX19TJ9SeahtxdYpfX19ox3CmOB6cB00uB7KJf7bJL0PmCRpC+BjwB/a3N4jkqZFxCJJ04BHB5sxImYBswBmzJgRXV1dbW1wQX/1HYm2G1unjPX4OsX14DpoqHs9lGnq+SiwFfB34FxgMfDxNrd3CXBQHj4IuLjN9ZiZWZvK9M75NHBMfgBLRER/mRVLOhfYCeiS9CBwLHAi8BNJhwH3A+9uN3AzM2vPkIlf0muA04EpebwPODQi5rZaLiL2HWTSzsMN0szMRk6ZNv7TgP8bEVcDSHoD6eEsW1cZmJmZVaNMG39/I+kDRMTvgVLNPWZmNvYMesQvads8eL2k75NO7AbwHuDK6kMzM7MqtGrq+VrT+LGF4eovjp/Aeo4aXtdHC0/co+31T58SQ17SOtz1m9n4Nmjij4g3dzIQMzPrjDJX9awLHEjqUfOf87tbZjOz8anMVT2/AK4DbgWeqzYcMzOrWpnEv1pEfKLySMzMrCPKXM75Q0kfkDRN0tTGq/LIzMysEmWO+P8BfAU4huev5glg86qCMjOz6pRJ/J8AXhwRY7fvYTMzK61MU8/twNNVB2JmZp1R5oh/KTBP0u9IXTMDvpzTzGy8KpP4f5ZfZmY2AZTpj3/2UPOYmdn4UebO3XsZoG+eiPBVPWZm41CZpp4ZheHVSE/N8nX8Zmbj1JBX9UTEY4XXQxHxDeBfOhCbmZlVoExTz7aF0ZVIvwCmVBaRmZlVqkxTT7Ff/iXAQmCfSqIxM7PKlbmqx/3yD2G4D1YxMxtNZZp6VgXeyfL98X++urDMzKwqZZp6Lgb6gLkU7tw1M7PxqUzi746IXSuPxMzMOqJMJ21/kPTKyiMxM7OOKHPE/wbg4HwH798BARERW1camZmZVaJM4t+t8ijMzKxjylzOeV8nAjEzs84o08ZvZmYTiBO/mVnNOPGbmdWME7+ZWc048ZuZ1YwTv5lZzZS5jn/ESVoI9ANLgSURMaP1EmZmNlJGJfFnb46I3lHcvplZLbmpx8ysZkYr8Qfwa0lzJc0cpRjMzGpptJp6Xh8RD0vaALhc0h0RMac4Q/5CmAnQ3d1Nb+/wW4X6+vqYPiVGJODRNNx9L+7zRmsMvf/t1O1409fXN9ohjDrXQeJ6GKXEHxEP57+PSroI2B6Y0zTPLGAWwIwZM6Krq6utbS3o14oFOwYMd9+b93moOmi3bsebuuxnK66DpO710PGmHklrSprSGAbeCtzW6TjMzOpqNI74NwQuktTY/o8i4lejEIeZWS11PPFHxD3Aqzq9XTMzS3w5p5lZzTjxm5nVjBO/mVnNjGaXDVZSz1E/H1PrX3jiHhVFYmad4CN+M7OaceI3M6sZJ34zs5px4jczqxknfjOzmnHiNzOrGSd+M7OaceI3M6sZ38BlHTHWbhIba/GYdZKP+M3MasaJ38ysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZnwDl5lZh7TzNL0qbh70Eb+ZWc048ZuZ1YwTv5lZzTjxm5nVjBO/mVnNOPGbmdWME7+ZWc34On4btnauRa7aUDFNnxIs6FeHounM9drD3UbVddCJh9X4ATojw0f8ZmY148RvZlYzTvxmZjXjxG9mVjNO/GZmNTMqiV/SrpLulHS3pKNGIwYzs7rqeOKXNAn4DrAbsCWwr6QtOx2HmVldjcYR//bA3RFxT0T8A/gxsPcoxGFmVkuKiM5uUHoXsGtEvD+PHwDsEBEfaZpvJjAzj74UuLONzXUBvSsQ7kTgOkhcD66DhjrVw6YRsX5z4WjcuTvQrYPLfftExCxg1gptSLoxImasyDrGO9dB4npwHTS4HkanqedBYJPCeDfw8CjEYWZWS6OR+G8AtpC0maRVgPcCl4xCHGZmtdTxpp6IWCLpI8BlwCTg9Ii4vaLNrVBT0QThOkhcD66DhtrXQ8dP7pqZ2ejynbtmZjXjxG9mVjMTMvFP9C4hJJ0u6VFJtxXKpkq6XNJd+e8LCtM+neviTklvK5RvJ+nWPO1kSZ17UskKkrSJpN9Jmi/pdklH5PLa1IOk1SRdL+lPuQ4+l8trUwcNkiZJulnSpXm8dnUwLBExoV6kE8YLgM2BVYA/AVuOdlwjvI//B9gWuK1Q9mXgqDx8FPClPLxlroNVgc1y3UzK064HdiTdW/FLYLfR3rdh1ME0YNs8PAX4S97X2tRDjnetPDwZ+CPw2jrVQaEuPgH8CLg0j9euDobzmohH/BO+S4iImAP8ral4b2B2Hp4NvL1Q/uOI+HtE3AvcDWwvaRqwdkRcG+m//qzCMmNeRCyKiJvycD8wH9iYGtVDJE/m0cn5FdSoDgAkdQN7AD8oFNeqDoZrIib+jYEHCuMP5rKJbsOIWAQpKQIb5PLB6mPjPNxcPu5I6gFeTTrirVU95CaOecCjwOURUbs6AL4B/AfwXKGsbnUwLBMx8ZfqEqJGBquPCVFPktYCLgA+HhGLW806QNm4r4eIWBoR25DugN9e0itazD7h6kDSnsCjETG37CIDlI3rOmjHREz8de0S4pH8c5X899FcPlh9PJiHm8vHDUmTSUn/nIi4MBfXrh4AIuIJ4EpgV+pVB68H/lXSQlKz7r9IOpt61cGwTcTEX9cuIS4BDsrDBwEXF8rfK2lVSZsBWwDX55+//ZJem69eOLCwzJiXYz4NmB8RJxUm1aYeJK0vad08vDqwC3AHNaqDiPh0RHRHRA/ps/7biNifGtVBW0b77HIVL2B30lUeC4BjRjueCvbvXGAR8CzpSOUwYD3gCuCu/HdqYf5jcl3cSeFKBWAGcFue9m3yndzj4QW8gfRT/BZgXn7tXqd6ALYGbs51cBvw2Vxemzpoqo+deP6qnlrWQdmXu2wwM6uZidjUY2ZmLTjxm5nVjBO/mVnNOPGbmdWME7+ZWc048Zu1QdJOjZ4g21j2cEkH5uGDJW00stGZtdbxRy+ajReSVo6IJSO93og4pTB6MOna8Ql7l6iNPT7itwlNUo+kOyTNlnSLpPMlrZH7Xr9K0lxJlxVu779S0hclXQUcIelMSadIulrSX3LfMM3bWFPpGQk35D7h987lJ0v6bB5+m6Q5klaSdJykT0p6F+mmoXMkzZO0h6SLCut9i6QLm7dntqKc+K0OXgrMioitgcXAh4FvAe+KiO2A04ETCvOvGxFvioiv5fEe4E2krn9PkbRa0/qPIXUV8BrgzcBXJK1J6gf+PZLeDJwMHBIR/+xBMiLOB24E9ovU0dovgJdLWj/PcghwxojUgFmBm3qsDh6IiGvy8NnA0cArgMvzQ5YmkbrAaDivafmf5IR9l6R7gJc1TX8rqaOwT+bx1YAXRcR8SR8A5gD/HhELWgUZESHph8D+ks4gPRTkwOHsqFkZTvxWB839kvQDt0fEjoPM/9QQyzePC3hnRNw5wLpeCTwGlD2Bewbw38AzwE+rOMdg5qYeq4MXSWok+X2B64D1G2WSJkvaqsXy785t89NJj/RsTvCXAR9tPKNV0qvz302BI0kPidlN0g4DrLuf9OhIACLiYdKJ3s8AZw5rL81KcuK3OpgPHCTpFmAquX0f+JKkP5F69nxdi+XvBK4iPYf18Ih4pmn68aTHHt4i6Tbg+EK30Z/Myfww4AcDnB84k3TeYF7uWhngHFLz1J/b212z1tw7p01o+bGMl0ZEqydTtVr+zLz8+SMY1lDb/DZwc0Sc1qltWr24jd9sDJE0l3SO4cjRjsUmLh/xm5nVjNv4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MauZ/AatLjAG7rw0TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.title(\"perplexity scores of sentences - histogram\")\n",
    "plt.hist([score for score in scores if score < 5000], bins = 25)\n",
    "plt.xlabel('perplexity')\n",
    "plt.ylabel('number of sentences')\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
