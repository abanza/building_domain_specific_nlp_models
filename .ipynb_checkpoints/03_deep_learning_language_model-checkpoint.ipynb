{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "POST_TYPE = 'post'\n",
    "MIN_TOKENS_LEN = 100\n",
    "MAX_TOKENS_LEN = 200\n",
    "DATA_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stackexchange_small_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape:  (77634, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"data.shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I have asked myself this question for months. The answers on CrossValidated and Quora all list nice properties of the logistic sigmoid function, but it all seems like we cleverly guessed this function. What I missed was the justification for choosing it. I finally found one in section . . . of the Deep Learning book by Bengio . In my own words In short, we want the logarithm of the model's output to be suitable for gradient-based optimization of the log-likelihood of the training data. Motivation We want a linear model, but we can't use .\"\n",
      " \"Problem is, the marginal likelihood is non-analytical due to the non-Gaussian observation likelihood, so you'll have to go with approximate inference I would try MCMC or variational Bayes here, due to the high uncertainty in the observations . This is the reason why I proposed a solution based on GPs on the bounds as opposed to a GP on the underlying function, since, with the former, one can use the standard analytical machinery. Anyhow, it is interesting to see how different approaches would work.\"]\n"
     ]
    }
   ],
   "source": [
    "print(data.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "data['tokens'] = data.tokens.apply(lambda token: np.array(token.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['excellent', 'book', ',', 'indeed', '!', 'however', ',', 'in',\n",
      "       'p.', ',', 'it', 'recommends', 'both', 'k', 'and', 'k', '.', 'why',\n",
      "       'would', 'you', 'recommend', 'k', 'for', 'my', 'case', '?'],\n",
      "      dtype='<U10')]\n"
     ]
    }
   ],
   "source": [
    "print(data.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of tokens:  4512785\n",
      "filtered number of tokens:  4386666\n",
      "vocabulary size: 11042\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary; filter out words that are too scare\n",
    "tokens_data = list(itertools.chain.from_iterable(data.tokens))\n",
    "\n",
    "# filter out least common tokens\n",
    "counter_tokens = Counter(tokens_data)\n",
    "\n",
    "vocab_size = len(set(tokens_data))\n",
    "vocab = list(set(tokens_data))\n",
    "\n",
    "# remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "filtered_tokens = [token for token in tokens_data if counter_tokens[token] > TOKENS_MIN_COUNT]\n",
    "\n",
    "vocab_size = len(set(filtered_tokens))\n",
    "vocab = list(set(filtered_tokens))\n",
    "\n",
    "print(\"initial number of tokens: \", len(tokens_data))\n",
    "print(\"filtered number of tokens: \", len(filtered_tokens))\n",
    "print(\"vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered out tokens\n",
    "filtered_out_tokens = np.unique([token for token in tokens_data if counter_tokens[token] <= TOKENS_MIN_COUNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'-.k\" \"'-b\" \"'-c\" ... 'ðŸ˜€' 'ðŸ˜Š' 'ðŸ˜”']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-group' 'themed' 'emerge' 'outs' 'grm.' 'hox' 'nonstochastic.'\n",
      " 'messes' 'rules-based' 'isometry' 's-curves.' 'afl' 'a\\\\end' 'min-hash'\n",
      " 'simplex.' 'ziggurat' 'steffen' 'layers.dense' 'dlogis' 'alba'\n",
      " 'satterwaite'\n",
      " 'plot-all-scatterplots-and-peak-those-with-biggest-white-area' 'autocode'\n",
      " 'd.v.' 'drivings' 'durring' 'discoveries' 'ommited' 'noah' 'datas.'\n",
      " 'classi-' 'ngene' 'p-vals' 'izenman' 'lookup.' 'punctuations'\n",
      " 'crash-introduction' 'imply.' 'mccaffrey' 're-posting' 'boss.'\n",
      " 'salakhutdinov' 'holds..' 'sentenceâ€¦' 'sprints' 'admm' 'young.' 'chernik'\n",
      " 'mineral' 'arching']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(filtered_out_tokens, 50, replace = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('UNKNOWN')\n",
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens as vocabulary indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {x: i for i, x in enumerate(vocab)}\n",
    "\n",
    "def get_index(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNKNOWN']\n",
    "    \n",
    "data['tokens_index'] = data.tokens.apply(lambda tokens: np.array([get_index(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 3908,  1479,  5602,  2856,  1005,  6551,  4523,  8577,   853,\n",
      "         259,   122,  1005,  1748,  6948,  4523,  7303,  2739,  3402,\n",
      "       10631,  1571,  4107,  1177,  1005,  3205,  3997, 10812, 10137,\n",
      "        9542,  6631,  5178,  6174, 11042,  7219,  9394,  4520,  1005,\n",
      "        5178,  6174, 11042,  1005,  8569,  8955,  9880,  7940,  5316,\n",
      "        3908,  2072,  9236,  5985,  1557,  2198,  2179,  2291,  2739,\n",
      "       11042, 11042,  9654])\n",
      " array([ 2142,  3967,  9236,  6894,  3967,  9236,  3402,  3242,  7219,\n",
      "        9253, 11042,  2906,  1479,  3402,  3242,  1512,  5316, 11042,\n",
      "        9058, 11042,  6336,  5742,  1596,  3908,  1614,  1479,  3913,\n",
      "        1005,  7298,  2142,  6948,  2406,  2739,  1716,  8255,  1479,\n",
      "        1005,  1005,  8779,  6479,  1198,  1479,  1005,  1005,  1005,\n",
      "        2906,  1479,  3402,  3242,  1512,  1005,  5122,  5770,  1479,\n",
      "        7283,  7006])\n",
      " array([ 3997,  4005,  6235,  9542,  4457,  6948,  2761,  6399, 11042,\n",
      "        8779,  9334,  2739,  9555,  3878,  4666,   101,  1479,  3322,\n",
      "        7395, 10257,  6902,  1005,  8840,  7395,  7938,  1479, 10232,\n",
      "        9542,  7793,  2906, 10438,  3373,   259,  7441,  4107,    18,\n",
      "         101,  1139,   101,  4537,  7299,  8840, 10716,  7749,  7219,\n",
      "        5178,  2344,  7569, 11042,   259, 10635,  2463,   805,  4785,\n",
      "        6948,   573,  2739,   101, 10457,   101,  7006])]\n"
     ]
    }
   ],
   "source": [
    "print(data.tokens_index.head(3).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generation(word):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(word) + SEQUENCE_WINDOW:\n",
    "        sequences.append(word[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "        \n",
    "    padded_seq = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sequence generation\n",
    "words_sequences = data.tokens_index.apply(sequence_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77634/77634 [07:30<00:00, 172.40it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x = 0\n",
    "for seq in tqdm(words_sequences.values):\n",
    "    if x == 0:\n",
    "        sequences = seq\n",
    "    else:\n",
    "        sequences = np.concatenate((sequences, seq))\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences.shape:  (1157748, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequences.shape: \", sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors.shape:  (1157748, 12)\n",
      "Label.shape:  (1157748,)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.6 GiB for an array with shape (1157748, 11043) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-799e51f2df58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# keras to_categorical function transform the vocab_size vector of labels into a one hot encoded matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# dimension (n, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlabel_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(\"Label_category.shape: \", label_category.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.7/site-packages/tensorflow/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 47.6 GiB for an array with shape (1157748, 11043) and data type float32"
     ]
    }
   ],
   "source": [
    "# predictors and labels for the classification task\n",
    "\n",
    "predictors = sequences[:, :-1]\n",
    "label = sequences[:, -1]\n",
    "\n",
    "print(\"Predictors.shape: \", predictors.shape)\n",
    "print(\"Label.shape: \", label.shape)\n",
    "\n",
    "# keras to_categorical function transform the vocab_size vector of labels into a one hot encoded matrix\n",
    "# dimension (n, vocab_size)\n",
    "label_category = to_categorical(label, num_classes=vocab_size)\n",
    "#print(\"Label_category.shape: \", label_category.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 64)            706752    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12, 11043)         1424547   \n",
      "=================================================================\n",
      "Total params: 2,230,115\n",
      "Trainable params: 2,230,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "dl_model = Sequential()\n",
    "dl_model.add(Embedding(vocab_size, embedding_dim, input_length=SEQUENCE_LEN -1))\n",
    "dl_model.add(LSTM(128, return_sequences=True))\n",
    "dl_model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "dl_model.compile(loss='categorical_crossentropy', \n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "print(dl_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-86c439092752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'label_cat' is not defined"
     ]
    }
   ],
   "source": [
    "# model fitting\n",
    "\n",
    "dl_model.fit(predictors, label_cat, batch_size = 256, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
