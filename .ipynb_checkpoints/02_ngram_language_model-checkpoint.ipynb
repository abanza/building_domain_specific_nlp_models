{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stackexchange_small_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77634, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id  parent_id  comment_id  \\\n",
      "0   229478        NaN    434340.0   \n",
      "1   411929        NaN         NaN   \n",
      "2   429144        NaN    800730.0   \n",
      "3   144104        NaN    275881.0   \n",
      "4   416728        NaN    777681.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  This is statistical terminology, not mathemati...  comment   \n",
      "1  Given formula for categorical formula for cros...     post   \n",
      "2  It's best to formulate the question very speci...  comment   \n",
      "3  Someone edited my question to add the negative...  comment   \n",
      "4  The plot is showing you a score for each varia...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  this is statistical terminology , not mathemat...        57  \n",
      "1  given formula for categorical formula for cros...        56  \n",
      "2  it 's best to formulate the question very spec...        61  \n",
      "3  someone edited my question to add the negative...        47  \n",
      "4  the plot is showing you a score for each varia...        21  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the tokens column as list \n",
    "data['tokens'] = data.tokens.apply(lambda token: token.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['it', 'would', 'be', 'ordinal', 'data', 'then']),\n",
       "       list(['i', 'will', 'treat', 'this', 'answer', 'in', 'two', 'parts', 'why', 'would', 'you', 'want', 'to', 'do', 'this', '?', 'to', 'begin', 'with', ',', 'variables', 'like', 'cost', 'and', 'travel', 'time', 'vary', 'across', 'alternatives', ',', 'which', 'actually', 'makes', 'them', 'generic', 'variables.', 'the', 'multinomial', 'logit', 'model', 'is', 'defined', 'on', 'the', 'difference', 'between', 'two', 'utility', 'functions.', 'say', 'you', 'have', 'two', 'alternatives', 'and', ',', 'where', 'alt', 'train', ',', ',', 'head', 'sm.mlogit', 'id', 'choice', 'tt', 'cost', 'id', 'alt', 'car', '.train', 'true', 'train', '.car', 'false', 'car', '.train', 'true', 'train', '.car', 'false', 'car', '.train', 'false', 'train', '.car', 'true', 'car', 'mnl', 'lt', '-', 'mlogit', 'choice', 'tt', 'i', 'cost', 'car', ',', 'data', 'sm.mlogit', 'summary', 'mnl', 'call', 'mlogit', 'formula', 'choice', 'tt', 'i', 'cost', 'car', ',', 'data', 'sm.mlogit', ',', 'method', 'nr', ',', 'print.level', 'frequencies', 'of', 'alternatives', 'car', 'train', '.', '.', 'nr', 'method', 'iterations', ',', 'h', 'm', 's', 'g', \"'\", '-h', '-', 'g', '.', 'e-', 'successive', 'function', 'values', 'within', 'tolerance', 'limits', 'coefficients', 'estimate', 'std.', 'error', 't-value', 'pr', 'gt', 't', 'train', 'intercept', '.', 'e', '.', 'e-', '.', 'lt', 'e-', 'tt', '.', 'e-', '.', 'e-', '.', '.', 'i', 'cost', 'car', '.', 'e-', '.', 'e-', '.', 'lt', 'e-', '--', '-', 'signif.', 'codes', \"'\", \"'\", '.', \"'\", \"'\", '.', \"'\", \"'\", '.', \"'.\", \"'\", '.', \"'\", \"'\", 'log-likelihood', '-', 'mcfadden', 'r', '.', 'likelihood', 'ratio', 'test', 'chisq', 'p.value', 'lt', 'e-']),\n",
       "       list(['your', 'hunch', 'about', 'what', 'is', 'going', 'on', 'is', 'on', 'the', 'right', 'track.', 'while', 'the', 't-test', 'approach', 'tests', 'each', 'contrast', 'against', 'its', 'own', 'separate', 'error', 'term', ',', 'the', 'rm-anova', 'tests', 'each', 'contrast', 'against', 'a', 'pooled', 'error', 'term', 'hence', 'df', ',', 'which', 'assumes', 'that', 'the', 'variances', 'of', 'the', 'contrast', 'scores', 'are', 'approximately', 'equal', 'i.e.', ',', 'sphericity', 'assumption', '.', 'the', 'results', 'from', 'these', 'two', 'methods', 'are', 'noticeably', 'different', 'here', 'because', ',', 'as', 'we', 'will', 'see', ',', 'the', 'variances', 'of', 'the', 'contrasts', 'are', 'very', 'clearly', 'unequal', ',', 'so', 'the', 'analysis', 'that', 'does', \"n't\", 'rely', 'on', 'sphericity', 'is', 'a', 'better', 'choice', 'here.', 'if', 'you', 'are', 'dead-set', 'on', 'pooling', 'the', 'error', 'terms', ',', 'for', 'example', 'in', 'order', 'to', 'test', 'the', '-df', 'trial', 'effect', ',', 'you', 'can', 'use', 'one', 'of', 'the', 'corrections', 'that', 'we', 'will', 'see', 'below.', 'before', 'moving', 'on', 'to', 'the', 'rest', 'of', 'the', 'explanation', ',', 'i', 'first', 'want', 'to', 'quickly', 'point', 'out', 'only', 'the', 'linear', 'contrast', 'is', 'comparable', 'between', 'your', 'two', 'analyses', ',', 'as', 'the', 'day', 'v', 'contrast', 'and', 'quad', 'contrasts', 'that', 'you', 'defined', 'are', 'clearly', 'not', 'equivalent.', 'first', 'let', \"'s\", 'replicate', 'the', 't-test', 'approach', 'using', 'a', 'multivariate', 'linear', 'model', 'mlm', 'assume', 'that', 'all', 'objects', 'from', 'your', 'example', 'code', 'are', 'loaded', 'already', 'conts', 'lt', '-', 'as.matrix', 'df.wide', ',', '-', 'contrasts', 'df.long', 'trial', '.l', '.q', ',', '-', '.', '-', '.', 'e-', ',', '-', '.', '-', '.', 'e-', ',', '-', '.', '-', '.', 'e', ',', '-', '.', '.', 'e-', ',', '-', '.', '.', 'e', ',', '-', '.', '.', 'e-', ',', '-', '.', '-', '.', 'e', ',', '-', '.', '-', '.', 'e', 'mod', 'lt', '-', 'lm', 'conts', 'summary', 'mod', 'response', '.l', 'call', 'lm', 'formula', '.l', 'residuals', 'min', 'q', 'median', 'q', 'max', '-', '.', '-', '.', '-', '.', '.', '.', 'coefficients', 'estimate', 'std.', 'error', 't', 'value', 'pr', 'gt', 't', 'intercept', '-', '.', '.', '-', '.', '.', '--', '-', 'signif.', 'codes', '‘', '’', '.', '‘', '’', '.', '‘', '’', '.', '‘.’', '.', '‘', '’', 'residual', 'standard', 'error', '.', 'on', 'degrees', 'of', 'freedom', 'response', '.q', 'call', 'lm', 'formula', '.q', 'residuals', 'min', 'q', 'median', 'q', 'max', '-', '.', '-', '.', '.', '.', '.', 'coefficients', 'estimate', 'std.', 'error', 't', 'value', 'pr', 'gt', 't', 'intercept', '-', '.', '.', '-', '.', '.', 'residual', 'standard', 'error', '.', 'on', 'degrees', 'of', 'freedom', 'now', 'let', \"'s\", 'see', 'what', 'it', 'looks', 'like', 'when', 'explicitly', 'using', 'the', 'pooled', 'error', 'term', 'approach', ',', 'just', 'to', 'demonstrate', 'its', 'equivalence', 'to', 'your', 'second', 'analysis', 'c', 'lt', '-', 'colsums', 'conts', '.l', '.q', 'these', 'are', 'the', 'variances', 'of', 'the', 'reduced', 'or', 'small', 'models', 'a', 'lt', '-', 'colsums', 'scale', 'conts', ',', 'scale', 'f', '.l', '.q', '.', '.', 'these', 'are', 'the', 'variances', 'of', 'the', 'full', 'or', 'large', 'models', 'these', 'are', 'what', 'is', 'being', 'pooled', 'together', 'to', 'form', 'the', 'error', 'term.', 'observe', 'sum', 'a', '.', 'this', 'is', 'the', 'pooled', 'residual', 'sum', 'sq', 'from', 'the', 'rm-anova', 'c', '-', 'a', '.l', '.q', '.', '.', 'and', 'these', 'are', 'the', 'ssrs', 'for', 'the', 'contrasts', 'in', 'the', 'rm-anova', 'now', 'let', \"'s\", 'run', 'the', 'model', 'specifying', 'that', 'we', 'are', 'assuming', 'sphericity', 'note', 'that', 'this', 'is', 'the', 'same', 'model', 'object', 'mod', 'from', 'the', 'first', 'block', 'of', 'code', 'above', 'anova', 'mod', ',', 'test', 'spherical', 'analysis', 'of', 'variance', 'table', 'greenhouse-geisser', 'epsilon', '.', 'huynh-feldt', 'epsilon', '.', 'df', 'f', 'num', 'df', 'den', 'df', 'pr', 'gt', 'f', 'g-g', 'pr', 'h-f', 'pr', 'intercept', '.', '.', '.', '.', 'residuals', 'note', 'the', 'equivalent', 'f-ratio', 'see', 'also', 'the', 'sphericity', 'tests', 'and', 'corrected', 'p-values']),\n",
       "       list(['ah', 'cool', 'i', 'see', 'the', 'difference', 'between', 'the', 'ways', 'of', 'doing', 'it', 'now.', 'so', 'even', 'for', 'only', 'groups', 'the', 'p-values', 'differ', 'because', 'summary', 'lm', '..', 'gives', 'you', 'the', 'p-value', 'for', 'intercept', 'i.e.', 'group', 'and', 'the', 'non-reference', 'group', 'i.e.', 'group', 'whereas', 'for', 'the', 'two', 'other', 'approaches', 'for', 'the', 'two', 'groups', 'you', 'get', 'an', 'overall', 'factor', 'effect.', 'is', 'that', 'the', 'right', 'conclusion', '?', 'and', 'for', 'two', 'groups', 'only', 'which', 'method', 'would', 'you', 'use', 'to', 'choose', 'the', 'p-value', 'from', '?', '?'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(4).tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test regimen\n",
    "train_data = data[data.category.isin(['post', 'comment'])].copy()\n",
    "test_data = data[data.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id  parent_id  comment_id  \\\n",
      "0   229478        NaN    434340.0   \n",
      "1   411929        NaN         NaN   \n",
      "2   429144        NaN    800730.0   \n",
      "3   144104        NaN    275881.0   \n",
      "4   416728        NaN    777681.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  This is statistical terminology, not mathemati...  comment   \n",
      "1  Given formula for categorical formula for cros...     post   \n",
      "2  It's best to formulate the question very speci...  comment   \n",
      "3  Someone edited my question to add the negative...  comment   \n",
      "4  The plot is showing you a score for each varia...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [this, is, statistical, terminology, ,, not, m...        57  \n",
      "1  [given, formula, for, categorical, formula, fo...        56  \n",
      "2  [it, 's, best, to, formulate, the, question, v...        61  \n",
      "3  [someone, edited, my, question, to, add, the, ...        47  \n",
      "4  [the, plot, is, showing, you, a, score, for, e...        21  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    post_id  parent_id  comment_id  \\\n",
      "21   268969        NaN         NaN   \n",
      "29    91177        NaN         NaN   \n",
      "37   219501        NaN         NaN   \n",
      "52   208495        NaN         NaN   \n",
      "58   288483        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "21         Is Gaussian AR Process Squared Stationary?    title   \n",
      "29  Machine learning techniques for spam detection...    title   \n",
      "37  What does it mean for two random variables to ...    title   \n",
      "52               Power of lady tasting tea experiment    title   \n",
      "58                How to average regression analyses?    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "21  [is, gaussian, ar, process, squared, stationar...         7  \n",
      "29  [machine, learning, techniques, for, spam, det...        13  \n",
      "37  [what, does, it, mean, for, two, random, varia...        12  \n",
      "52        [power, of, lady, tasting, tea, experiment]         6  \n",
      "58        [how, to, average, regression, analyses, ?]         6  \n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ngrams count dict and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69549/69549 [00:10<00:00, 6672.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "counts = defaultdict(Counter)\n",
    "\n",
    "for tokens in tqdm(train_data.tokens.values):\n",
    "    for ngram in ngrams(tokens, n = trigrams, \n",
    "                        pad_right = True, pad_left = True,\n",
    "                       left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        prefix = ngram[:trigrams-1]\n",
    "        token = ngram[trigrams-1]\n",
    "        counts[prefix][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams count\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('different', 'location'): \n",
      "Counter({'for': 2, 'of': 1, 'types': 1, 'image': 1})\n",
      "('batch', 'lt'): \n",
      "Counter({'-': 1})\n",
      "('total', 'numbers.'): \n",
      "Counter({'have': 1})\n",
      "('since', 'the'): \n",
      "Counter({'data': 27, 'sample': 14, 'variance': 10, 'op': 9, 'last': 9, 'number': 9, 'first': 9, 'probability': 8, 'model': 7, 'question': 7, 'two': 7, 'distribution': 7, 'latter': 6, 'problem': 6, 'beginning': 5, 'values': 5, 'exponential': 4, 'test': 4, 'outcome': 4, 'mean': 4, 'original': 4, 'standard': 4, 'variables': 4, 'population': 4, 'former': 3, 'ratio': 3, 'time': 3, 'pdf': 3, 'true': 3, 'initial': 3, 'sum': 3, 'new': 3, 'outputs': 3, 'input': 3, 'network': 3, 'transformation': 3, 'p-value': 3, 'dataset': 3, 'correlation': 3, 'class': 2, 'majority': 2, 'resulting': 2, 'response': 2, 'chi-square': 2, 'estimates': 2, 'variable': 2, 'x': 2, 'negative': 2, 'random': 2, 'samples': 2, 'learning': 2, 'slopes': 2, 'terms': 2, 'results': 2, 'matrix': 2, 'predictor': 2, 'precision': 2, 'posterior': 2, 'analysis': 2, 'features': 2, 'joint': 2, 'threshold': 2, 'estimate': 2, 'shape': 2, 'degrees': 2, 'authors': 2, 'whole': 2, 'average': 2, 'statistic': 2, 'design': 2, 't-distribution': 2, 'penalty': 2, 'general': 2, 'highest': 2, 'gradient': 2, 'answer': 2, 'change': 2, 'previous': 2, 'difference': 2, 'intercept': 2, 'assumption': 2, 'start': 2, 'density': 2, 'value': 2, 'state': 2, 't-test': 2, 'residual': 2, 'support': 2, 'logarithm': 2, 'interval': 2, 'median': 2, 'computer': 2, 'gaussian': 2, 'r': 2, 'effect': 2, 'default': 2, 'cumulative': 1, 'above': 1, 'tosses': 1, 'real': 1, 'transitions': 1, 'focus': 1, 'thing': 1, 'pre-': 1, 'lowest': 1, 'denominator': 1, 'work': 1, 'booking': 1, 'models': 1, 'is': 1, 'amount': 1, 'uncertain': 1, 'quality': 1, 'link': 1, 'multilevel': 1, 'numerator': 1, 'assumptions': 1, 'absolute': 1, 'residuals': 1, 'vector': 1, 'deviation': 1, 'instrument': 1, 'popuation': 1, 'underlying': 1, 'people': 1, 'rotation-criterion': 1, 'plot': 1, 'integrand': 1, 'stopping': 1, 'means': 1, 'scales': 1, 'curve': 1, 'conditional': 1, 'corresponding': 1, 'mle': 1, 'algorithm': 1, 'section': 1, 'definition': 1, 'procedure': 1, 'aic': 1, 'process': 1, 'coefficient': 1, 'lrs': 1, 'computation': 1, 'image': 1, 'blue': 1, 'text': 1, 'sentences': 1, 'code': 1, 'coefficients': 1, 'amounts': 1, 'time-series': 1, 'are': 1, 'power': 1, 'quantity': 1, 'folds': 1, 'dirichlet': 1, 'covariance': 1, 'late': 1, 'common': 1, 'dependent': 1, 'credible': 1, 'knee': 1, \"'neither\": 1, 'nd': 1, 'covariates': 1, 'plotted': 1, 'group': 1, 'lsmeans': 1, 'weight': 1, 'rnn': 1, 'hypotheses': 1, 'radius': 1, 'fa': 1, 'article': 1, 'very': 1, 'requirement': 1, 'rd': 1, 'verb': 1, 'theory': 1, 'mixture': 1, 'required': 1, 'heteroskedasticity': 1, 'baseline': 1, 'proportions': 1, 'differences': 1, 'tests': 1, 'counts': 1, 'durbin-watson': 1, 'lasso': 1, 'em': 1, 'vast': 1, 'participant': 1, 'maximal': 1, 'aim': 1, 'embeddings': 1, 'conditioning': 1, 'series': 1, 'autocorrelation': 1, 'normal': 1, 'daily': 1, 'function': 1, 'background': 1, 'standardizing': 1, 'nn': 1, 'questions': 1, 'studied': 1, 'proportionality': 1, 'expb': 1, 'linear': 1, 'interaction': 1, 'interactions': 1, 'predicted': 1, 'p': 1, 'hai': 1, 'information': 1, 'method': 1, 'changes': 1, 'estimator': 1, 'wilcoxon': 1, 'increase': 1, 'in': 1, 'only': 1, 'n': 1, 'but': 1, 'hessian': 1, 'terminal': 1, 'maximum': 1, 'rules': 1, 'order': 1, 'optimal': 1, 'measurements': 1, 'more': 1, 'square': 1, 'meandecreaseaccuracy': 1, 'chain': 1, 'longest': 1, 'stream': 1, 'treatment': 1, 'equations': 1, 'author': 1, 'signal': 1, 'inverse-fourier': 1, 'objective': 1, 'deviations': 1, 'properties': 1, 'rule': 1, 'can': 1, 'situation': 1, 'mode': 1, 'wedge': 1, 'result': 1, 'beta': 1, 'questionnaires': 1, 'hazard': 1, 'best': 1, 'loss': 1, 'activity': 1, 'vectors': 1, 'nyquist': 1, 'player': 1, 'business': 1, 'monthly': 1, 'errors': 1, 'optimum': 1, 'acf': 1, 'pacf': 1, 'spikes': 1, 'all': 1, 'overwhelming': 1, 'important': 1, 'details': 1, 'dependence': 1, 'tested': 1, 'generated': 1, 'natural': 1, 'frequencies': 1, 'curves': 1, 'inequation': 1, 'null': 1, 'glm': 1, 'vc': 1, 'video': 1, 'log': 1, 'ground': 1, 'rnns': 1, 'target': 1, 'issue': 1, 'higher-order': 1, 'bootstrap': 1, 'rv': 1, 'trillionaires': 1, 'boot': 1, 'total': 1, 'diameter': 1, 'genome': 1, 'seminal': 1, 'context': 1, 'rejection': 1, 'classic': 1, 'duplicate': 1, 'individuals': 1, 'noise': 1, 'other': 1, 'formula': 1, 'combination': 1, 'neurons': 1, 'arm': 1, 'parameters': 1, 'cases': 1, 'readouts': 1, 'dimensionality': 1, 'positive': 1, 'second': 1, 'fisher': 1, 'expected': 1, 'continuous': 1, 'rest': 1, 'proposed': 1, 'final': 1, 'prior': 1, 'deterministic': 1, 'speed': 1, 'algorithms': 1, 'full': 1, 'kullback-leibler': 1, 'user': 1, 'metric': 1, 'records': 1, 'discuss': 1, 'cox': 1, 'marginal': 1, 'half-normal': 1, 'type': 1, 'clt': 1, 'binomial': 1, 'angle': 1, 'fixed': 1, 'update': 1, 'markov': 1, 'error': 1, 'dimension': 1, 'predictors': 1, 'heart': 1, 'ranking': 1, 'cards': 1, 'totals': 1, 'numerical': 1, 'flow': 1, 'moderator': 1, 'output': 1, 'variation': 1, 'actual': 1, 'search': 1, 'study': 1, 'regression': 1, 'seed': 1, 'dawn': 1, 'delta': 1, 'y': 1, 'matched': 1, 'area': 1, \"'s\": 1, 'classifier': 1, 'dual': 1})\n",
      "('defining', '.'): \n",
      "Counter({'see': 1, '</s>': 1})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for key in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print('{}: \\n{}'.format(prefix, counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count for each bigram\n",
    "\n",
    "tokens_count = [len(value) for key, value in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "plt.hist(tokens_count, bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625023\n"
     ]
    }
   ],
   "source": [
    "# bigram with one token\n",
    "bigram_with_one_token = [key for key, value in counts.items() if len(value) == 1]\n",
    "print(len(bigram_with_one_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105522\n"
     ]
    }
   ],
   "source": [
    "# bigram with two tokens\n",
    "bigram_with_two_tokens = [key for key, value in counts.items() if len(value) == 2]\n",
    "print(len(bigram_with_two_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefix-token probabilites\n",
    "For each prefix-token count, we can divide the count by the total number of the prefix occurence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_freq = defaultdict(dict)\n",
    "\n",
    "for prefix, tokens in counts.items():\n",
    "    prefix_total = sum(counts[prefix].values())\n",
    "    for token, count in tokens.items():\n",
    "        prefix_freq[prefix][token] = count / prefix_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ordinal', 'catgeorical'): {'dependent': 1.0}\n",
      "('meta.cv', '.'): {'</s>': 1.0}\n",
      "('relevance', 'in'): {'the': 1.0}\n",
      "('anything', 'reliable'): {'from': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for item in range(4):\n",
    "    prefix = random.choice(list(prefix_freq.keys()))\n",
    "    print(\"{}: {}\".format(prefix, prefix_freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n_words = 50):\n",
    "    for item in range(n_words):\n",
    "        prefix = tuple(text.split()[-trigrams + 1:])\n",
    "        \n",
    "        if len(prefix_freq[prefix]) == 0:\n",
    "            break\n",
    "            \n",
    "        candidates = list(prefix_freq[prefix].keys())\n",
    "        probas = list(prefix_freq[prefix].values())\n",
    "        text += ' ' + np.random.choice(candidates, p =probas)\n",
    "        \n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can any bigrams as long as it is in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "the model is of no value. if bins are not as sigma x - . . . . . </s>\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'the model'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "that distribution than it is mostly linear algebra multiplication . it is not an expert in the new formula i.e. the simple question about what look like inv. gaussian. do you mean by 'centered ' makes you think that you want a proper prior , i.e. better than others </s>\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'that distribution'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "to determine what price ? </s>\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'to determine'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature(text, temperature = 1, n_words = 50):\n",
    "    \n",
    "    for item in range(n_words):\n",
    "        prefix = tuple(text.split()[-trigrams + 1:])\n",
    "        \n",
    "        if len(prefix_freq[prefix]) == 0:\n",
    "            break\n",
    "            \n",
    "        candidates = list(prefix_freq[prefix].keys())\n",
    "        initial_probas = list(prefix_freq[prefix].values())\n",
    "        \n",
    "        denom = sum([p ** temperature for p in initial_probas])\n",
    "        probas = [p ** temperature / denom for p in initial_probas]\n",
    "        text += ' ' + np.random.choice(candidates, p = probas)\n",
    "        \n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the temperature, the less chaotic the generated text will end up be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model typically has three unknown variables other than binary classifiers . you apparently forgetting that these functions are simple intensity comparisons , e.g. a random guessing ? the f comes after , as stated , does this help my question remains using what we observe the cause then after there shapiro-wilk\n",
      "0.5\n",
      "the model may have completely understood your question without reference to lm.fit.sparse . i think that anybody can give contradictory information. what i require. thank you so that ? perhaps between ? is there some edge detection performed better on their formulation , which branch would i go another step that to\n",
      "1\n",
      "the model for prediction in arma models require modelling residuals as capturing the true risk \\\\ amp -p o ab p o ab p a np.random.rand p , but would greatly appreciated ! </s>\n",
      "3\n",
      "the model , and the other hand , the variance of the data , and the other hand , the model , and it is not a good idea , but i am not sure if that 's not a good idea to use the same as the dependent variable is a\n",
      "10\n",
      "the model is not a good idea to use the same as the number of observations , and the other hand , if you have a look at the end of the data is not a good idea to use the same as the number of observations , and the other hand\n"
     ]
    }
   ],
   "source": [
    "text = 'the model'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "    print(tau)\n",
    "    print(generate_temperature(text, temperature=tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Let us measure the quality of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize  import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    n = len(sentence)\n",
    "    logprob = 0\n",
    "    \n",
    "    for ngram in ngrams(sentence, n = trigrams, \n",
    "                        pad_right = True, pad_left = True,\n",
    "                       left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        try:\n",
    "            prefix = ngram[:trigrams - 1]\n",
    "            token = ngram[trigrams - 1]\n",
    "            logprob += np.log(prefix_freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return np.exp(-logprob / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the perplexity on some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 14.10] the difference between the two approaches is discussed here\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 13.39] the model typically has three unknown variables other than binary classifiers\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the model typically has three unknown variables other than binary classifiers\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 20.20] that distribution than it is mostly linear algebra multiplication\n"
     ]
    }
   ],
   "source": [
    "sentence = \"that distribution than it is mostly linear algebra multiplication\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
