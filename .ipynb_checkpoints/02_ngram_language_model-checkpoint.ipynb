{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stackexchange_small_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77634, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id  parent_id  comment_id  \\\n",
      "0   229478        NaN    434340.0   \n",
      "1   411929        NaN         NaN   \n",
      "2   429144        NaN    800730.0   \n",
      "3   144104        NaN    275881.0   \n",
      "4   416728        NaN    777681.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  This is statistical terminology, not mathemati...  comment   \n",
      "1  Given formula for categorical formula for cros...     post   \n",
      "2  It's best to formulate the question very speci...  comment   \n",
      "3  Someone edited my question to add the negative...  comment   \n",
      "4  The plot is showing you a score for each varia...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  this is statistical terminology , not mathemat...        57  \n",
      "1  given formula for categorical formula for cros...        56  \n",
      "2  it 's best to formulate the question very spec...        61  \n",
      "3  someone edited my question to add the negative...        47  \n",
      "4  the plot is showing you a score for each varia...        21  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the tokens column as list \n",
    "data['tokens'] = data.tokens.apply(lambda token: token.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['how', 'did', 'you', 'make', 'those', 'plots', 'such', 'that', 'they', 'are', 'side', 'and', 'side', 'and', 'in', 'one', 'image', '?']),\n",
       "       list(['obtaining', 'a', 'desired', 'average', 'with', 'a', 'minimum', 'constraint']),\n",
       "       list(['there', 'were', 'no', 'characters', 'left', 'people', 'say', 'this', 'prior', 'is', 'noninformative', 'because', 'it', 'assigns', 'the', 'same', 'probability', 'to', 'each', 'possible', 'value', 'of', 'the', 'parameter', ',', 'and', 'i', 'think', 'this', 'a', 'mistake', 'to', 'consider', 'that', 'uniformity', 'means', 'noninformativeness', '-', 'in', 'passing', 'this', 'would', 'imply', 'that', 'the', 'prior', 'on', 'the', 'odd', 'parameter', 'p', '-p', 'is', 'informative', 'since', 'it', 'is', 'not', 'uniform', '?', 'this', 'is', 'not', 'coherent', 'every', 'distribution', 'is', 'uniform', 'up', 'to', 'a', 'transformation', '.']),\n",
       "       list(['yes', ',', 'of', 'course.', 'how', 'else', 'would', 'we', 'know', 'how', 'much', 'better', 'we', 'do', 'after', 'a', 'partiular', 'split', '?', 'to', 'that', 'extent', ',', 'certain', 'boosting', 'algorithm', 'that', 'employ', 'regression', 'as', 'their', 'base', 'learners', 'e.g.', 'lightgbm', 'have', 'a', 'minimum', 'gain', 'to', 'split', 'attribute', 'when', 'training', 'exactly', 'so', 'they', 'regularise', 'splits', 'that', 'might', 'overfit', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(4).tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test regimen\n",
    "train_data = data[data.category.isin(['post', 'comment'])].copy()\n",
    "test_data = data[data.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id  parent_id  comment_id  \\\n",
      "0   229478        NaN    434340.0   \n",
      "1   411929        NaN         NaN   \n",
      "2   429144        NaN    800730.0   \n",
      "3   144104        NaN    275881.0   \n",
      "4   416728        NaN    777681.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  This is statistical terminology, not mathemati...  comment   \n",
      "1  Given formula for categorical formula for cros...     post   \n",
      "2  It's best to formulate the question very speci...  comment   \n",
      "3  Someone edited my question to add the negative...  comment   \n",
      "4  The plot is showing you a score for each varia...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [this, is, statistical, terminology, ,, not, m...        57  \n",
      "1  [given, formula, for, categorical, formula, fo...        56  \n",
      "2  [it, 's, best, to, formulate, the, question, v...        61  \n",
      "3  [someone, edited, my, question, to, add, the, ...        47  \n",
      "4  [the, plot, is, showing, you, a, score, for, e...        21  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    post_id  parent_id  comment_id  \\\n",
      "21   268969        NaN         NaN   \n",
      "29    91177        NaN         NaN   \n",
      "37   219501        NaN         NaN   \n",
      "52   208495        NaN         NaN   \n",
      "58   288483        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "21         Is Gaussian AR Process Squared Stationary?    title   \n",
      "29  Machine learning techniques for spam detection...    title   \n",
      "37  What does it mean for two random variables to ...    title   \n",
      "52               Power of lady tasting tea experiment    title   \n",
      "58                How to average regression analyses?    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "21  [is, gaussian, ar, process, squared, stationar...         7  \n",
      "29  [machine, learning, techniques, for, spam, det...        13  \n",
      "37  [what, does, it, mean, for, two, random, varia...        12  \n",
      "52        [power, of, lady, tasting, tea, experiment]         6  \n",
      "58        [how, to, average, regression, analyses, ?]         6  \n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ngrams count dict and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69549/69549 [00:10<00:00, 6669.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "counts = defaultdict(Counter)\n",
    "\n",
    "for tokens in tqdm(train_data.tokens.values):\n",
    "    for ngram in ngrams(tokens, n = trigrams, \n",
    "                        pad_right = True, pad_left = True,\n",
    "                       left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        prefix = ngram[:trigrams-1]\n",
    "        token = ngram[trigrams-1]\n",
    "        counts[prefix][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams count\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('indicator', 'variable'): \n",
      "Counter({'for': 4, 'and': 2, 'method': 2, 'that': 2, 'to': 2, 'of': 1, 'on': 1, 'approach': 1, ',': 1, 'in': 1, 'might': 1, 'is': 1, 'with': 1, 'seems': 1, 'bounded': 1, 'was': 1, 'matrix': 1, 'which': 1, 'as': 1, 'has': 1, 'critical': 1})\n",
      "('threshold', 'need'): \n",
      "Counter({'to': 1})\n",
      "('exactpoici', 'lt'): \n",
      "Counter({'-': 1})\n",
      "('the', 'wooldridge'): \n",
      "Counter({'lecture': 1})\n",
      "('significant', 'were'): \n",
      "Counter({'other': 1})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for key in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print('{}: \\n{}'.format(prefix, counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count for each bigram\n",
    "\n",
    "tokens_count = [len(value) for key, value in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "plt.hist(tokens_count, bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625023\n"
     ]
    }
   ],
   "source": [
    "# bigram with one token\n",
    "bigram_with_one_token = [key for key, value in counts.items() if len(value) == 1]\n",
    "print(len(bigram_with_one_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105522\n"
     ]
    }
   ],
   "source": [
    "# bigram with two tokens\n",
    "bigram_with_two_tokens = [key for key, value in counts.items() if len(value) == 2]\n",
    "print(len(bigram_with_two_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefix-token probabilites\n",
    "For each prefix-token count, we can divide the count by the total number of the prefix occurence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_freq = defaultdict(dict)\n",
    "\n",
    "for prefix, tokens in counts.items():\n",
    "    prefix_total = sum(counts[prefix].values())\n",
    "    for token, count in tokens.items():\n",
    "        prefix_freq[prefix][token] = count / prefix_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('logit.gre', 'glm'): {'admit': 1.0}\n",
      "('just', 'overall'): {'outcomes': 1.0}\n",
      "('correlation.', 'with'): {'the': 0.5, 'n': 0.5}\n",
      "('auc', 'can'): {'be': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for item in range(4):\n",
    "    prefix = random.choice(list(prefix_freq.keys()))\n",
    "    print(\"{}: {}\".format(prefix, prefix_freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n_words = 50):\n",
    "    for item in range(n_words):\n",
    "        prefix = tuple(text.split()[-trigrams + 1:])\n",
    "        \n",
    "        if len(prefix_freq[prefix]) == 0:\n",
    "            break\n",
    "            \n",
    "        candidates = list(prefix_freq[prefix].keys())\n",
    "        probas = list(prefix_freq[prefix].values())\n",
    "        text += ' ' + np.random.choice(candidates, p =probas)\n",
    "        \n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can any bigrams as long as it is in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "the model we are in need of generating correlated data. however , as shown above. especially since . hamilton 's for example if i would recommend tree-based systems human readable and analogical it is just my opinion , this is the not seasonally adjusted and non-seasonally adjusted format and not done any\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'the model'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "that distribution function . </s>\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'that distribution'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "to determine the best way of thinking of applying any test. also , i think the message you want . </s>\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "text = 'to determine'\n",
    "print('*' * 20)\n",
    "print(generate(text))\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature(text, temperature = 1, n_words = 50):\n",
    "    \n",
    "    for item in range(n_words):\n",
    "        prefix = tuple(text.split()[-trigrams + 1:])\n",
    "        \n",
    "        if len(prefix_freq[prefix]) == 0:\n",
    "            break\n",
    "            \n",
    "        candidates = list(prefix_freq[prefix].keys())\n",
    "        initial_probas = list(prefix_freq[prefix].values())\n",
    "        \n",
    "        denom = sum([p ** temperature for p in initial_probas])\n",
    "        probas = [p ** temperature / denom for p in initial_probas]\n",
    "        text += ' ' + np.random.choice(candidates, p = probas)\n",
    "        \n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the temperature, the less chaotic the generated text will end up be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model which accounts for longitudinal analysis c it affects your answer. why would your argument would be infinite. whatever the distance decay if it took to mean assuming a balanced -way anova on conversion rates we get - . or when a call for . error is about named distributions in\n",
      "0.5\n",
      "the model training process. i 've seen many times. i do to someone asking what is small we can all be more of theoretical. i am mixing the two categories , because p-value refers to an ideal answer to start but no definition , uses various exponential smoothing ? </s>\n",
      "1\n",
      "the model , you could try to find minimum faster , for in-general sample-distributions . </s>\n",
      "3\n",
      "the model ? </s>\n",
      "10\n",
      "the model is not a good idea to use the same as the number of observations , but i am not sure if this is a good idea to use the same as the number of observations , the model is the same as the number of observations , but i do\n"
     ]
    }
   ],
   "source": [
    "text = 'the model'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "    print(tau)\n",
    "    print(generate_temperature(text, temperature=tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Let us measure the quality of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize  import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    \n",
    "    for ngram in ngrams(sentence, n = trigrams, \n",
    "                        pad_right = True, pad_left = True,\n",
    "                       left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        try:\n",
    "            prefix = ngram[:trigrams - 1]\n",
    "            token = ngram[trigrams - 1]\n",
    "            logprob += np.log(prefix_freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return np.exp(-logprob / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the perplexity on some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 14.10] the difference between the two approaches is discussed here\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 13.39] the model typically has three unknown variables other than binary classifiers\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the model typically has three unknown variables other than binary classifiers\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 20.20] that distribution than it is mostly linear algebra multiplication\n"
     ]
    }
   ],
   "source": [
    "sentence = \"that distribution than it is mostly linear algebra multiplication\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_with_laplace(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    \n",
    "    for ngram in ngrams(sentence, n = trigrams,\n",
    "                       pad_right=True, pad_left=True,\n",
    "                       left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:trigrams - 1]\n",
    "        token = ngram[trigrams - 1]\n",
    "        \n",
    "        if prefix in list(counts.keys()):\n",
    "            total_counts = sum(counts[prefix].values())\n",
    "            if token in counts[prefix].values():\n",
    "                logprob += np.log((counts[prefix][token] + delta) / (total_counts + delta * N))\n",
    "            else:\n",
    "                logprob += np.log((delta) / (total_counts + delta * N))\n",
    "        else:\n",
    "            logprob += -np.log(N)\n",
    "    return np.exp(-logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 213.99] the difference between the two approaches is discussed here\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_with_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 66.29] the model typically has three unknown variables other than binary classifiers\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the model typically has three unknown variables other than binary classifiers\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_with_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 84.47] that distribution than it is mostly linear algebra multiplication\n"
     ]
    }
   ],
   "source": [
    "sentence = \"that distribution than it is mostly linear algebra multiplication\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_with_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity on the test corpus and sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculates the probability of a sentence\n",
    "\n",
    "def logproba_of_sentence(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logproba = 0\n",
    "    \n",
    "    for ngram in ngrams(sentence, n = trigrams, \n",
    "                       pad_right=True, pad_left=True,\n",
    "                       left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:trigrams - 1]\n",
    "        token = ngram[trigrams - 1]\n",
    "        try:\n",
    "            logproba += np.log(prefix_freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "    return logproba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculates the perplexity on whole set of sentences\n",
    "\n",
    "def corpus_perplexity(corpus):\n",
    "    corpus_sentences = ' '.join(corpus)\n",
    "    \n",
    "    corpus_tokens = tokenizer.tokenize(corpus_sentences.lower())\n",
    "    N = len(tokens)\n",
    "    \n",
    "    logproba = 0\n",
    "    \n",
    "    for sentence in tqdm(corpus):\n",
    "        logproba += logproba_of_sentence(sentence)\n",
    "        \n",
    "    return np.exp(-logproba / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8382.30it/s]\n",
      "/home/adelard/anaconda3/envs/allennlp/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity of a 500 sample of titles\n",
    "corpus = test_data.text.sample(1000, random_state=42).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8085/8085 [00:00<00:00, 9536.98it/s]\n",
      "/home/adelard/anaconda3/envs/allennlp/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity of the whole test corpus\n",
    "corpus_perplexity(test_data.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an n-gram language model with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stackexchange_small_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the tokens column as list \n",
    "data['tokens'] = data.tokens.apply(lambda token: token.split())\n",
    "\n",
    "# creating train and test regimen\n",
    "train_data = data[data.category.isin(['post', 'comment'])].copy()\n",
    "test_data = data[data.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>434340.0</td>\n",
       "      <td>This is statistical terminology, not mathemati...</td>\n",
       "      <td>comment</td>\n",
       "      <td>[this, is, statistical, terminology, ,, not, m...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Given formula for categorical formula for cros...</td>\n",
       "      <td>post</td>\n",
       "      <td>[given, formula, for, categorical, formula, fo...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800730.0</td>\n",
       "      <td>It's best to formulate the question very speci...</td>\n",
       "      <td>comment</td>\n",
       "      <td>[it, 's, best, to, formulate, the, question, v...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275881.0</td>\n",
       "      <td>Someone edited my question to add the negative...</td>\n",
       "      <td>comment</td>\n",
       "      <td>[someone, edited, my, question, to, add, the, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>416728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>777681.0</td>\n",
       "      <td>The plot is showing you a score for each varia...</td>\n",
       "      <td>comment</td>\n",
       "      <td>[the, plot, is, showing, you, a, score, for, e...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   229478        NaN    434340.0   \n",
       "1   411929        NaN         NaN   \n",
       "2   429144        NaN    800730.0   \n",
       "3   144104        NaN    275881.0   \n",
       "4   416728        NaN    777681.0   \n",
       "\n",
       "                                                text category  \\\n",
       "0  This is statistical terminology, not mathemati...  comment   \n",
       "1  Given formula for categorical formula for cros...     post   \n",
       "2  It's best to formulate the question very speci...  comment   \n",
       "3  Someone edited my question to add the negative...  comment   \n",
       "4  The plot is showing you a score for each varia...  comment   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [this, is, statistical, terminology, ,, not, m...        57  \n",
       "1  [given, formula, for, categorical, formula, fo...        56  \n",
       "2  [it, 's, best, to, formulate, the, question, v...        61  \n",
       "3  [someone, edited, my, question, to, add, the, ...        47  \n",
       "4  [the, plot, is, showing, you, a, score, for, e...        21  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "padded_train_data = [ngrams(t, n = trigrams, pad_right=True, pad_left=True,\n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in train_data.tokens.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for sent in train_data.tokens for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "vocab = Vocabulary(words, unk_cutoff=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = MLE(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(padded_train_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "train_data['tokens'] = train_data.text.apply(lambda token: tokenizer.tokenize(token.lower()))\n",
    "\n",
    "padded_line = [list(pad_both_ends(train_data, n=2))]\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, padded_line)\n",
    "\n",
    "lm = MLE(2)\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 21 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
