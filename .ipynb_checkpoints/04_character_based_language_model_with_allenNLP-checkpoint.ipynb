{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-based language model with AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp\n",
    "print(allennlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch libraries\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# standard libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Set, Iterable\n",
    "\n",
    "# AllenNLP libraries\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.dataloader import DataLoader\n",
    "from allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, CharacterTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary, DEFAULT_PADDING_TOKEN\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('/home/adelard/ml/manning/dataset/stackexchange_full_data_tokenized.csv.gz',\n",
    "                  compression='gzip').sample(frac = 1, random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string into a list of characters\n",
    "text = ''.join(full_data.text.values).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [char for char in text]\n",
    "unique_characters = np.unique(characters)\n",
    "print(len(unique_characters))\n",
    "print()\n",
    "print(unique_characters[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vocabulary by keeping the most common characters in the dataset\n",
    "char_count = Counter(characters)\n",
    "print(len(char_count.most_common()))\n",
    "print(char_count.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_VOCAB_SIZE = 50\n",
    "MAX_VOCAB_SIZE = 50\n",
    "chars_most_common = [char[0] for char in char_count.most_common(MAX_VOCAB_SIZE)]\n",
    "chars_most_common.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the dataset \n",
    "POSTS_TYPE = 'title'\n",
    "SAMPLE_COUNT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample the full dataset\n",
    "small_data = full_data[(full_data.category == POSTS_TYPE)].sample(SAMPLE_COUNT).reset_index(drop=True)\n",
    "print(\"data shape: \", small_data.shape)\n",
    "print(small_data.text.sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllenNLP structure\n",
    "#### Instances\n",
    "An instance composed of the input and output objects.\n",
    "#### Model\n",
    "Model uses a forward() method that takes tensor inputs and produces a dict of tensor outputs that includes the loss used to train the model.\n",
    "#### Tokenizer and instances\n",
    "The AllenNLP CharacterTokenizer is used to splitting the sentences into list of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set is composed of tokenized sequences of the original text\n",
    "train_data = small_data.text.apply(lambda txt: tokenizer.tokenize(txt.lower())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a list of tokens and an indexer as input and returns \n",
    "# an instance composed of the input and output tokens.\n",
    "\n",
    "def tokens_to_instance(toks: List[Token], token_indexers: Dict[str, TokenIndexer]):\n",
    "    tokens = list(toks)\n",
    "    tokens.insert(0, Token(START_SYMBOL))\n",
    "    tokens.append(Token(END_SYMBOL))\n",
    "    \n",
    "    input_field = TextField(tokens[:-1], token_indexers)\n",
    "    output_field = TextField(tokens[1:], token_indexers)\n",
    "    return Instance({'input_tokens': input_field, 'output_tokens': output_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SingleIdTokenIndexer takes care of the mapping between the\n",
    "# tokens and their unique index in the vocabulary. \n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "instances = [tokens_to_instance(tokens, token_indexers) for tokens in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = {char: 1 for char in chars_most_common}\n",
    "vocab = Vocabulary({'tokens': token_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(Model):\n",
    "    \n",
    "    def __init__(self, embedder: TextFieldEmbedder,\n",
    "                hidden_size: int,\n",
    "                max_len: int,\n",
    "                vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        self.embedder = embedder\n",
    "        \n",
    "        # initialize a seq2seq encodeer, LSTM\n",
    "        self.rnn = PytorchSeq2SeqWrapper(\n",
    "        torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n",
    "        \n",
    "        self.hidden2out = torch.nn.Linear(in_features=self.rnn.get_output_dim(),\n",
    "                                         out_features=vocab.get_vocab_size('tokens'))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, input_tokens, output_tokens):\n",
    "        mask = get_text_field_mask(input_tokens)\n",
    "        embeddings = self.embedder(input_tokens)\n",
    "        rnn_hidden = self.rnn(embeddings, mask)\n",
    "        out_logits = self.hidden2out(rnn_hidden)\n",
    "        loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)\n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def generate_text(self)-> Tuple[List[Token], torch.tensor]:\n",
    "        \n",
    "        start_symbol_index = self.vocab.get_token_index(START_SYMBOL, 'tokens')\n",
    "        end_symbol_index = self.vocab.get_token_index(END_SYMBOL, 'tokens')\n",
    "        padding_symbol_index = self.vocab.get_token_indexn(DEFAULT_PADDING_TOKEN, 'tokens')\n",
    "        \n",
    "        log_likelihood = 0.\n",
    "        result_words = []\n",
    "        state = (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "        \n",
    "        word_index = start_symbol_index\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            tokens = torch.tensor([[word_index]])\n",
    "            \n",
    "            embeddings = self.embedder({'tokens': tokens})\n",
    "            output, state = self.rnn._module(embeddings, state)\n",
    "            output = self.hidden2out(output)\n",
    "            \n",
    "            log_prob = torch.log_softmax(output[0, 0], dim=0)\n",
    "            \n",
    "            dist = torch.exp(log_prob)\n",
    "            word_index = start_symbol_index\n",
    "            \n",
    "            while word_index in {start_symbol_index, padding_symbol_index}:\n",
    "                word_index = torch.multinomial(dist, num_samples=1, replacement=False).item()\n",
    "                \n",
    "            log_likelihood += log_prob[word_index]\n",
    "            \n",
    "            if word_index == end_symbol_index:\n",
    "                break\n",
    "                \n",
    "            token = Token(text-self.vocab.get_token_from_index(word_index, 'tokens'))\n",
    "            words.append(token)\n",
    "            \n",
    "        return result_words, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the embeddings, the rnn model, the optimizer\n",
    "# the trainer\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n",
    "                            embedding_dim=EMBEDDING_SIZE)\n",
    "embedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "model = RNNModel(embedder=embedder, hidden_size=HIDDEN_SIZE, max_len=80, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AllennlpDataset(instances, vocab)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "loader = next(iter(loader))\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=5.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                 optimizer=optimizer,\n",
    "                 iterator=loader,\n",
    "                 train_dataset=instances_tokens,\n",
    "                 num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(txt: str, model: Model) -> float:\n",
    "    tokenizer = CharacterTokenizer()\n",
    "    tokens = tokenizer.tokenizer(txt)\n",
    "    \n",
    "    token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "    instance = tokens_to_instance(tokens, token_indexers)\n",
    "    output = model.forward_on_instance(instance)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(25):\n",
    "    tokens, _ = model.generate()\n",
    "    print(''.join(token.text for token in tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
