{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and Preparing the Dataset\n",
    "Language is very domain specific. Businesses collect text-based datasets that are tailored to their domains (legal, healthcare, insurance, social networks, finance, etc.). These domain-specific corpuses can then be exploited in multiple ways: entity recognition, search indexing, query completion, product recommendations systems, or sentiment analysis, to name just a few.\n",
    "\n",
    "Language models are at the core of many natural language processing (NLP) applications like the ones listed in the previous paragraph. Simply put, given surrounding or preceding words, a language model predicts a word. As you will see, you can directly exploit language models for any application that generates text, such as machine translation, speech to text, text generation, or query completion applications.\n",
    "\n",
    "Off-the-shelf models which are trained on large generic corpuses do not reflect the particularities of a given business domain and corpus. For instance, completing the query \"how to avoid over…\" will not give the same results in the general public domain (overeating, overthinking, …) compared to a data science context where “overfitting” would be a more appropriate completion.\n",
    "\n",
    "To reap the benefits of domain-specific corpuses, we must build language models that are tuned to the particular vocabulary of the domain at hand. Since Stack Exchange operates domain specific forums, the content of these forums constitutes perfect corpuses for building domain specific language models.\n",
    "\n",
    "In this Project, you will be taking on the role of an NLP data scientist at Stack Exchange, a network of question-and-answer (Q&A) websites on topics in diverse fields. Stack Exchange has over 10M registered users and is best known for its flagship websites StackOverflow or AskUbuntu. You will build statistics-focused language models using gradually more complex methods. You will evaluate and apply these models to the tasks of:\n",
    "\n",
    "- Query completion\n",
    "- Larger text generation\n",
    "- Sentence selection\n",
    "\n",
    "At the end of this project, you will be able to build the foundations of any domain-specific NLP system by creating the most a robust and efficient language model.\n",
    "\n",
    "The link to the dataset is available here:\n",
    "\n",
    "https://go.aws/30Vpl5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Loading and preparing the dataset\n",
    "### Objective\n",
    "\n",
    "- The goal of this preliminary task is to load and clean the dataset. The raw text is noisy and we want to remove nonwords, keep punctuation to a minimum, and reduce the overall vocabulary of the corpus.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Load the dataset into a pandas dataframe.\n",
    "2. Use regular expressions to remove elements that are not words such as HTML tags, LaTeX expressions, URLs, digits, line returns, and so on.\n",
    "3. Remove missing values for texts\n",
    "4. Remove texts that are extremely large or too short to bring any information to the model. We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables.\n",
    "5. Use a tokenizer to create a version of the original text that is a string of space-separated lowercase tokens. For instance,\n",
    "\n",
    "Thank you!, This equation y = ax + by=ax+b, is very helpful.\n",
    "would be transformed as:\n",
    "thank you ! this equation , is very helpful.\n",
    "\n",
    "“retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.\n",
    "becomes, if you choose to remove double quotes from the original text:\n",
    "retrieve a distance matrix is a matter of coding . it also might be irrelevant : one can imagine creative answers .\n",
    "\n",
    "- Note that punctuation signs (, . : !) are also represented as tokens.\n",
    "Export the resulting dataframe into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Regular expressions\n",
    "A regular expression (regex) is a way of recognizing and often extracting data from certain patterns of text. A regex that recognizes a piece of text or a string is said to match that text or string. A regex is defined by a string in which certain characters (the so-colled metacharacters) can have a special meaning, which enables a single regex to match many different specific strings. \n",
    "\n",
    "It's easier to understand this through example than through explanation. Here's a program with a regular expression taht counts how many lines in a text file contain the word hello. A line that contains hello more than once is counted only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regexp = re.compile(\"[hH]ello\")\n",
    "count = 0\n",
    "file = open(\"textfile\", 'r')\n",
    "\n",
    "for line in file.readlines():\n",
    "    if regexp.search(line):\n",
    "        count = count + 1\n",
    "file.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops, I don't think this is a record\n",
      "Oops, I don't think this is a record\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regexp = re.compile(r\"(?P<last>[-a-zA-Z]+),\"\n",
    "         r\" (?P<first>[-a-zA-Z]+)\"\n",
    "         r\"( (?P<middle>([-a-zA-Z]+)))?\"\n",
    "         r\": (?P<phone>(\\(\\d{3}-)?\\d{3}-\\d{4})\"\n",
    "         )\n",
    "\n",
    "file = open(\"textfile\", 'r')\n",
    "\n",
    "for line in file.readlines():\n",
    "    result = regexp.search(line)\n",
    "    if result == None:\n",
    "        print(\"Oops, I don't think this is a record\")\n",
    "    else:\n",
    "        lastname = result.group('last')\n",
    "        firstname = result.group('first')\n",
    "        middlename = result.group('middle')\n",
    "        \n",
    "        if middlename == None:\n",
    "            middlename = \"\"\n",
    "        phonenumber = result.group('phone')\n",
    "    #print('Name:', firstname, middlename, lastname, ' Number:', phonenumber)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Tokenization\n",
    "Tokenization is a process where the input text is split into smaller units. There are two types of tokenization: word and sentence tokenization. Word tokenization splits a sentence into tokens (rough equivalent of words and punctuations) that I mentioned above. Sentence tokenization, on the other hand, splits a piece of text that may include more than one sentence into individual sentences. If you just say tokenization, it usually means word tokenization in NLP.\n",
    "\n",
    "Many NLP libraries and frameworks support tokenization out of the box, because it is one of the most fundamental and widely used pre-processing steps in NLP. In what follows, I’d like to show you how to do tokenization using two popular NLP libraries—NLTK (https://www.nltk.org/) and spaCy (https://spacy.io/).\n",
    "\n",
    "Note that the results from NLTK and spaCy are slightly different from each other. For example, spaCy’s word tokenizer leaves newlines ('\\n') intact. The behavior of tokenizers differs from one implementation to another, and there is no single standard solution that every NLP practitioner agrees upon. Although standard libraries such as NLTK and spaCy give a good baseline, be ready to experiment depending on your task and data. Also, if you are dealing with languages other than English, your options may vary (and might be quite limited depending on the language). If you are familiar with the Java ecosystem, Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/) is another good NLP framework worth checking out.\n",
    "\n",
    "Finally, there is an increasingly popular and important tokenization method for neural network-based NLP models called byte-pair encoding (BPE). Byte-pair encoding is a purely statistical technique to split text into sequences of characters in any language, not relying on heuristic rules (such as spaces and punctuations) but only on character statistics from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me two of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK implements a wide range of tokenizers in addition to the default one we used here. Its documentation page (https://www.nltk.org/api/nltk.tokenize.html) is a good starting point if you are interested in exploring more options.\n",
    "\n",
    "You can tokenize words and sentences as follows using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " '\\n',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " ' ',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " '\\n',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " ' ',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results from NLTK and spaCy are slightly different from each other. For example, spaCy’s word tokenizer leaves newlines ('\\n') intact. The behavior of tokenizers differs from one implementation to another, and there is no single standard solution that every NLP practitioner agrees upon. Although standard libraries such as NLTK and spaCy give a good baseline, be ready to experiment depending on your task and data. Also, if you are dealing with languages other than English, your options may vary (and might be quite limited depending on the language). If you are familiar with the Java ecosystem, Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/) is another good NLP framework worth checking out.\n",
    "\n",
    "Finally, there is an increasingly popular and important tokenization method for neural network-based NLP models called byte-pair encoding (BPE). Byte-pair encoding is a purely statistical technique to split text into sequences of characters in any language, not relying on heuristic rules (such as spaces and punctuations) but only on character statistics from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_vectors_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df = pd.read_csv('dataset/stackexchange_812k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_text = str(stack_df['text']).lower()\n",
    "stack_df['text_tokens'] = stack_df.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_tokens = word_tokenize(stack_text)\n",
    "lines = [str(l) for l in stack_df.text_tokens.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df.text_tokens = [list(nlp(t)) for t in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df.to_csv(\"stackexchange.csv\", index=False, sep='\\t', \n",
    "                encoding='utf-8', columns=['text', 'text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "      <td>[eliciting, priors, from, experts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "      <td>[what, is, normality, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "      <td>[what, are, some, valuable, statistical, analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "      <td>[assessing, the, significance, of, differences...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "      <td>[the, two, cultures, :, statistics, vs., machi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \\\n",
       "0                      Eliciting priors from experts    title   \n",
       "1                                 What is normality?    title   \n",
       "2  What are some valuable Statistical Analysis op...    title   \n",
       "3  Assessing the significance of differences in d...    title   \n",
       "4  The Two Cultures: statistics vs. machine learn...    title   \n",
       "\n",
       "                                         text_tokens  \n",
       "0                 [eliciting, priors, from, experts]  \n",
       "1                           [what, is, normality, ?]  \n",
       "2  [what, are, some, valuable, statistical, analy...  \n",
       "3  [assessing, the, significance, of, differences...  \n",
       "4  [the, two, cultures, :, statistics, vs., machi...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
